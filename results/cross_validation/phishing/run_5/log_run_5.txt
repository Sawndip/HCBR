# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,736] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
0.00233777 
0.0623089 
0.00149868 
0.00158749 
0.0697486 
6.86425e-05 
0.000241889 
0.000553343 
0.008029 
5.99733e-05 
0.000238773 
0.00377711 
0.00205713 
0.0542218 
0.00278445 
0.031837 
0.000914962 
0 
0.000317959 
0.00143623 
0.000193541 
0.0089683 
0.0440485 
4.96227e-05 
0.000162313 
0.00968773 
0.00129907 
0.000865223 
0.0040685 
0.00299188 
0.00469214 
2.64091e-05 
0.00124313 
0.000131559 
0.00220984 
0.00331034 
0.00464441 
0.00831821 
0.00110753 
0.00289713 
0.000231471 
0.00247035 
0.000130327 
0.00237148 
6.26786e-07 
5.75347e-05 
0.000389229 
0.000219418 
0.000697523 
0.0118684 
0.000352884 
0 
0.000287371 
0.00148797 
0.00717298 
0 
0.00119208 
0.000123038 
0.0071375 
0.00119753 
0.000130972 
0.000122978 
0.0014243 
0.000371904 
1.94856e-05 
0.00143687 
0.00623448 
0.00345711 
0.00412464 
0.0255978 
0.0235919 
0.00128484 
0.000693831 
0.00173132 
0.00539048 
0.000379713 
0.00665855 
0.00329178 
0.00831337 
0.0149646 
0.00434355 
0.000650837 
0.000253612 
3.69865e-05 
0.00165125 
0.000529977 
0.00197632 
0.00328507 
0.00262028 
0.00331488 
0.0193136 
4.06827e-05 
0.000776221 
0.00193555 
0.0104769 
0.00396052 
0.00026552 
0.00087821 
0.000895134 
0.00210315 
0.000928546 
0.00182136 
0.000195729 
0.00121366 
0.000292021 
0.00153836 
0.000774715 
0.0105278 
0.00573892 
0.00122731 
0.0112258 
0.00066408 
0.000441957 
0.000142204 
0.000974158 
0.000301945 
0.000106654 
1.73234e-06 
0.00891625 
0.00201098 
0.00257273 
0.00466946 
0.00105559 
0.000502016 
0.000348188 
0.000250956 
0.000728924 
0.00162849 
0.00819556 
0.00135442 
0.00180027 
0.00904123 
0.000365025 
0.00343176 
0.00145852 
0.000996896 
0.00215404 
0.0132781 
0.00174025 
0.00267652 
3.42908e-07 
0.000299596 
0.000218222 
0.0184654 
0.000523522 
0 
0.000688894 
0.000270098 
0.00040881 
0.00532803 
0.00243624 
0.000473778 
9.88927e-05 
0.000194513 
0.00254914 
0.00246707 
0.00105601 
0.00140464 
0.000245728 
1.61613e-05 
0.00122979 
0.00135458 
0.000108923 
0.00126946 
0.00379747 
0.00167152 
0.000325161 
0.000154233 
0.000785824 
0.000396182 
0.00225614 
0.00183753 
0.000946481 
0.000404182 
0.000145293 
0.000810673 
0.000700562 
0.000481507 
0.000131645 
0.000268859 
0.00421883 
9.48743e-05 
0.000140664 
0.0031813 
0.0014077 
0.00915496 
7.86865e-05 
0.000721648 
0.000472302 
0.000722875 
0.00156128 
1.87762e-05 
0.00128957 
0.000359566 
0.000286097 
7.46463e-06 
3.68882e-07 
0 
0.00403104 
0.00101909 
0.00506567 
0.000904143 
0.00485015 
0.000510546 
0.00182293 
6.48008e-05 
0.00435732 
0.000852777 
0.00248321 
0.000372122 
0.00118015 
0.00232354 
0.00230978 
0.000365521 
0.00165122 
0.000382751 
0.000839079 
0.000979647 
0.000652199 
0.0056281 
0.000124612 
0.00733138 
0.000155495 
2.6159e-05 
4.21767e-06 
0.000146945 
0.000232702 
0.00226939 
0.000425567 
0.000178084 
0.00520006 
0.000114668 
0.00029794 
0.000494223 
0.000212385 
0.000233241 
3.78533e-05 
5.94606e-05 
0.000322797 
0.00256101 
0.000530844 
0.000203355 
0.000276892 
0.000110616 
0.00111202 
0.000686801 
0.00310431 
0.000490052 
0.00185687 
0.00274699 
0.000300029 
6.67176e-05 
0.000506514 
5.83206e-05 
0.000148536 
0.00928993 
0.000656078 
7.39047e-05 
1.6253e-05 
8.71241e-05 
4.07202e-05 
0.000177002 
0.000117353 
0.000178752 
0.00781463 
0.000266528 
0.00150664 
0.000421958 
0.000362926 
0.000341753 
1.37973e-05 
8.61908e-05 
0.000476591 
0.00808106 
0.00307281 
0.000735314 
0.00012788 
0.000365064 
0.000510888 
0.000219629 
0.000512943 
0.000177262 
0.00351485 
0.000576072 
0.000132274 
4.07921e-06 
3.8869e-05 
5.65696e-05 
0.00377983 
5.95872e-05 
4.68363e-06 
1.66096e-05 
1.9483e-05 
0.000290016 
8.36313e-05 
0.0161756 
0.000387046 
8.51603e-05 
2.46628e-05 
8.98203e-05 
0.00245396 
0.000682366 
0.000288876 
0.00204256 
0.00350176 
4.83865e-05 
0.00010963 
0.00071782 
2.85152e-05 
9.55653e-05 
0.000550589 
0 
0.00028283 
2.20756e-05 
2.13556e-06 
0.000813012 
0.000246875 
0.000365986 
0.000226662 
6.69405e-05 
0.000687347 
0.000710344 
7.96292e-06 
0.000148787 
0.000220596 
4.94734e-05 
7.14387e-05 
5.57522e-05 
5.93203e-06 
0.000256622 
4.22795e-06 
0.00837515 
0.00872343 
0.000798388 
0.000415126 
0.000834092 
0.00655607 
0.000284772 
5.09603e-06 
6.43055e-05 
0.000946495 
0.000135655 
0.000718423 
3.40139e-05 
0.000383745 
0.000715135 
5.63288e-05 
0.000671002 
0.000701173 
6.76606e-05 
0.000136976 
7.22921e-05 
1.33076e-07 
1.71859e-05 
1.52377e-05 
2.10503e-05 
3.12571e-05 
9.36313e-06 
1.02503e-05 
1.9665e-05 
1.89085e-05 
9.24431e-05 
6.8898e-06 
0.000131667 
4.32475e-05 
0.000120837 
0.000516113 
2.82963e-06 
0.0099296 
4.7858e-05 
0.000134479 
0.000267524 
2.23814e-06 
0.00055295 
4.4607e-05 
0.000157357 
7.50664e-05 
4.87119e-06 
5.52613e-07 
3.29315e-05 
2.52411e-05 
0 
0.000546847 
0.000120905 
5.38009e-05 
0.000154379 
9.79999e-06 
6.04362e-05 
0.000262255 
2.18674e-05 
2.25585e-06 
1.78584e-05 
4.15069e-05 
7.78438e-05 
9.24182e-06 
1.44082e-06 
0.00166046 
0.000205511 
4.07743e-05 
3.69977e-05 
0.000290877 
3.20803e-05 
3.50075e-05 
4.97182e-05 
5.61217e-05 
6.77474e-07 
5.56314e-05 
0.000423721 
0.00013919 
0.000174652 
5.30719e-05 
1.69899e-05 
3.60227e-05 
0.000183965 
4.71403e-05 
0.00862316 
0.00497713 
0.000236635 
0.000246056 
8.13684e-06 
5.29699e-05 
1.64177e-05 
8.30782e-05 
4.44636e-07 
5.39818e-05 
3.73932e-05 
6.14209e-05 
9.3899e-07 
7.77846e-05 
4.82048e-05 
4.57645e-05 
1.87489e-05 
9.15345e-05 
7.66011e-05 
3.44265e-05 
1.97906e-05 
5.53749e-06 
1.21174e-06 
2.8228e-06 
9.23936e-05 
1.37003e-05 
8.84666e-07 
0.00997959 
0.000168408 
0.000818111 
6.45362e-05 
7.76603e-05 
0.000110347 
6.48341e-05 
0.000167401 
5.49305e-05 
9.85604e-05 
9.56402e-05 
2.53898e-05 
0.000162506 
5.36671e-05 
3.73642e-05 
4.11028e-06 
3.74513e-05 
0.000140088 
1.34155e-05 
3.01584e-05 
1.61025e-06 
5.37992e-07 
1.96302e-05 
8.22021e-06 
8.53615e-06 
1.64677e-05 
1.34547e-05 
3.67209e-06 
0.000275024 
1.80401e-06 
1.66751e-05 
3.16683e-05 
2.68889e-05 
2.78945e-06 
1.14721e-06 
2.19443e-06 
9.08997e-05 
0 
1.00328e-05 
3.07955e-05 
7.56154e-06 
0 
0.00595735 
0.00035487 
4.48277e-05 
1.72653e-06 
0.000102118 
1.44561e-05 
6.18263e-05 
9.82238e-07 
1.74287e-06 
1.73911e-06 
0.0026275 
0.00269645 
3.57255e-06 
0.000474624 
2.80204e-05 
1.17359e-05 
6.27117e-05 
9.51234e-06 
8.35863e-05 
7.87004e-05 
8.78581e-07 
2.84739e-06 
0 
2.08106e-06 
1.11018e-06 
0 
6.21437e-05 
0.000118669 
4.65879e-06 
3.13158e-05 
3.13337e-05 
2.84505e-08 
1.17934e-05 
2.84505e-08 
1.38075e-05 
2.32305e-05 
9.17409e-07 
6.50856e-06 
1.05781e-06 
6.47354e-06 
2.93426e-05 
1.46089e-05 
1.6676e-05 
8.15099e-07 
4.73806e-06 
9.70415e-06 
1.10496e-06 
3.71408e-06 
8.53667e-07 
3.15457e-06 
2.16368e-05 
1.48916e-05 
5.62243e-07 
1.22553e-06 
4.37854e-06 
3.2888e-05 
0.000470309 
5.51329e-05 
0.000233122 
8.61606e-07 
1.60218e-06 
0.000800904 
0.000108837 
5.5571e-06 
3.93369e-06 
1.94393e-05 
3.61974e-05 
3.93449e-07 
1.05897e-06 
3.16557e-05 
1.12513e-06 
1.14378e-05 
5.32196e-06 
3.88833e-07 
4.60077e-05 
5.02205e-05 
3.78532e-07 
5.61142e-05 
3.12746e-05 
3.82436e-05 
7.88328e-07 
1.51297e-05 
2.14902e-05 
3.44314e-05 
3.49465e-05 
3.23284e-06 
3.63872e-07 
1.88349e-06 
0 
3.47565e-07 
6.04502e-06 
0 
6.89483e-06 
1.9164e-06 
3.43382e-06 
7.68812e-07 
1.40196e-06 
6.26786e-07 
0.00492569 
0.00857605 
2.46727e-05 
8.10173e-05 
1.15562e-05 
9.14262e-05 
1.87953e-07 
4.4021e-05 
2.58067e-05 
9.72997e-06 
8.94318e-06 
1.04189e-06 
6.28877e-07 
2.25653e-08 
0 
0 
0.000124875 
1.47756e-06 
4.04692e-08 
1.6039e-06 
5.51748e-06 
0.000621424 
3.50803e-06 
8.04084e-06 
5.81576e-08 
6.81315e-05 
0 
1.40618e-07 
6.21142e-07 
0 
4.81972e-07 
0 
1.40483e-06 
6.9099e-07 
0 
2.70567e-06 
1.33658e-06 
1.31389e-05 
4.21321e-05 
8.21246e-06 
1.21299e-07 
1.11051e-08 
8.21246e-06 
4.40446e-07 
7.47599e-06 
3.90751e-06 
1.04627e-06 
4.6619e-07 
5.16661e-06 
0.00288099 
1.41929e-06 
1.91554e-06 
1.15187e-05 
0 
8.20937e-07 
1.91617e-07 
6.7021e-06 
0 
7.44294e-05 
7.33136e-08 
2.1632e-08 
3.8214e-08 
9.14026e-06 
7.3201e-08 
2.73143e-07 
2.86756e-07 
1.28428e-06 
2.75599e-06 
3.26548e-07 
3.62678e-05 
2.79302e-06 
0 
0.000816238 
4.06307e-07 
7.98672e-07 
0 
1.76791e-05 
2.33962e-05 
0 
2.25653e-08 
4.46796e-07 
1.91617e-07 
4.88263e-07 
3.44334e-07 
8.10118e-07 
4.84108e-08 
2.86756e-07 
1.76502e-06 
4.5881e-06 
8.90064e-07 
2.67067e-06 
2.96549e-06 
7.43045e-08 
0 
0 
1.59469e-06 
2.43937e-06 
2.73143e-07 
5.57833e-07 
2.40686e-07 
3.37787e-05 
0 
1.5521e-07 
9.89517e-08 
3.07074e-06 
1.50031e-05 
4.73806e-06 
4.47503e-07 
1.41929e-06 
6.69756e-05 
0.000237403 
0 
3.59638e-06 
9.50837e-07 
0.000817512 
1.24832e-06 
8.0187e-07 
6.26786e-07 
1.60131e-07 
6.28877e-07 
1.76502e-06 
6.28877e-07 
2.26396e-05 
9.17304e-08 
0 
6.96745e-07 
1.57871e-07 
1.14702e-06 
0 
6.28877e-07 
0 
1.4119e-05 
2.62721e-07 
1.54006e-08 
0 
3.52976e-06 
0 
2.47425e-05 
1.2088e-05 
0 
3.52976e-06 
4.7168e-07 
0 
2.86756e-07 
0 
0 
0 
6.28877e-07 
0 
6.28877e-07 
1.8811e-08 
1.56655e-07 
3.92866e-07 
0 
# Learning phase...
 - Before training
Accuracy: 7177/9950 = 0.721307
Average error toward 0: -0.000188302 (1801)
Average error toward 1: 7.66492e-05 (972)
Prev: 0.443216 (error: 0.181005) Offset: 0 0
Ratio error 1 : 0.350523
-----------------
- 3739 - 972 - 
-----------------
- 1801 - 3438 - 
-----------------
 - Phase 1
Accuracy: 8901/9950 = 0.894573
Average error toward 0: -0.000200 (532)
Average error toward 1: 0.000086 (517)
Prev: 0.443216 (error: 0.053467) Offset: 0.000000 0.000000
Ratio error 1 : 0.492850
-----------------
- 5008 - 517 - 
-----------------
- 532 - 3893 - 
-----------------
 - Phase 2
Accuracy: 9182/9950 = 0.922814
Average error toward 0: -0.000202 (391)
Average error toward 1: 0.000088 (377)
Prev: 0.443216 (error: 0.039296) Offset: 0.000000 0.000000
Ratio error 1 : 0.490885
-----------------
- 5149 - 377 - 
-----------------
- 391 - 4033 - 
-----------------
 - Phase 3
Accuracy: 9212/9950 = 0.925829
Average error toward 0: -0.000203 (368)
Average error toward 1: 0.000088 (370)
Prev: 0.443216 (error: 0.036985) Offset: 0.000000 0.000000
Ratio error 1 : 0.501355
-----------------
- 5172 - 370 - 
-----------------
- 368 - 4040 - 
-----------------
 - Phase 4
Accuracy: 9263/9950 = 0.930955
Average error toward 0: -0.000203 (341)
Average error toward 1: 0.000089 (346)
Prev: 0.443216 (error: 0.034271) Offset: 0.000000 0.000000
Ratio error 1 : 0.503639
-----------------
- 5199 - 346 - 
-----------------
- 341 - 4064 - 
-----------------
 - Phase 5
Accuracy: 9260/9950 = 0.930653
Average error toward 0: -0.000204 (345)
Average error toward 1: 0.000089 (345)
Prev: 0.443216 (error: 0.034673) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 5195 - 345 - 
-----------------
- 345 - 4065 - 
-----------------
 - Verification
Accuracy: 9269/9950 = 0.931558
Average error toward 0: -0.000204 (495)
Average error toward 1: 0.000090 (186)
Prev: 0.443216 (error: 0.049749) Offset: 0.000000 0.000000
Ratio error 1 : 0.273128
-----------------
- 5045 - 186 - 
-----------------
- 495 - 4224 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 775 0.980645
0.941290
Accuracy: 1072/1104 = 0.971014
Average error toward 0: -0.000000 (18)
Average error toward 1: 0.000000 (14)
Prev: 0.443216 (error: 0.001809) Offset: 0.000000 0.000000
Ratio error 1 : 0.437500
-----------------
- 599 - 14 - 
-----------------
- 18 - 473 - 
-----------------
# Prediction serialization...
# Saving the [1104,736] weight matrix...
4.625820
