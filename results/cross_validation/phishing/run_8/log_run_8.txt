# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,738] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
5.84732e-05 
0.000246997 
1.64374e-05 
0.00662002 
0.00291308 
0.00235098 
0 
5.10629e-05 
0.00714175 
0.00989733 
0.00012874 
0.000660222 
0.000102785 
0.0431627 
0.000227366 
1.0181e-05 
0.00137322 
0.00802977 
0.00906341 
1.52009e-05 
0.00819351 
0.068977 
0.000605932 
0.000401175 
0.001728 
0.00411302 
0.00538865 
4.99463e-05 
0.0149329 
0.00102556 
0.00298113 
0.000963706 
0.0541673 
0.00319381 
0.00106747 
0.00145566 
0.00663498 
0.000807134 
0.00797991 
0.000227391 
0.00336072 
0.00363149 
0.00468572 
0.0627658 
0.00298678 
0.00176071 
0.00127571 
0.0232647 
0.0183417 
7.43192e-05 
0.00683043 
0.000626593 
0.000179587 
0.0020696 
0.00570463 
0.0246828 
0.00491757 
0.00470079 
0.00173881 
0.00108895 
0.00110069 
0.00403623 
0.0160445 
0.0109014 
0.0198049 
0.000906306 
0.0324191 
0.00425199 
0.000281738 
0.00795259 
0.0025294 
0.00235555 
0.0116635 
0.00893586 
0.000369418 
0.00089808 
0.000124746 
0.000302573 
0.000522151 
0.00171582 
0.0109513 
0.00249633 
0.00625727 
0.00856096 
0.00917623 
0.000294787 
0.0100739 
0.00038838 
0.000115915 
0.000923077 
0.000475952 
0.00332916 
0.00140811 
0.00863955 
0.000107711 
0.00141215 
0.00359 
0.000274164 
0.00939781 
0.0102909 
0.0105725 
0.000362792 
1.45603e-05 
0.000243722 
0.00161668 
0.0136002 
0.00502764 
0.00576842 
0.00767431 
0.00877479 
0.000508205 
0.000433684 
0.00125436 
0.00231917 
0.00372737 
2.74644e-05 
0.000392934 
0.000683368 
0.00108648 
5.5994e-05 
0.000730877 
0.00110563 
0.000380336 
0.000381229 
0.00564934 
0.000389646 
0.000198143 
0.00278655 
0.00417429 
0.000114249 
0.00131342 
0.000416529 
0.000871041 
0.00192376 
0.00271695 
0.000369061 
0.000660788 
0.000111299 
0.00871849 
8.38536e-05 
0.000599776 
0 
5.75603e-05 
0.000722227 
0.00438053 
0.000751361 
0.00268789 
0.00159372 
0.000506236 
2.43722e-05 
0.000921941 
0.00173826 
0.00376684 
0.00126373 
0.000213546 
3.82465e-05 
0.00976602 
8.53522e-05 
0.000496622 
0.000298377 
0.000452791 
1.07952e-05 
0.00147866 
0.00480066 
0.00362435 
0.00287854 
0.00085563 
0.000286491 
0.000242125 
0.0022515 
0.00295149 
0.000747256 
0.00256793 
0.00114887 
0.000684988 
0.00216873 
0.000890822 
0.000338712 
0.00251017 
0.0018451 
0.000142382 
0.00018902 
0.00013857 
0.00121107 
0.000227397 
0.00315447 
9.99033e-06 
2.20342e-06 
9.33818e-05 
0.000506163 
0.000474106 
0.000186184 
0.00245473 
0.000193441 
0.000363622 
0.000716782 
3.31478e-05 
0.000303554 
0.0012223 
8.22676e-06 
0.000773252 
0.00319357 
0.00254501 
0.00221156 
0.00192691 
0.00210913 
0.00483843 
0.0006992 
0.000848691 
0.000267245 
0.000551835 
0 
0.00201747 
0.00159744 
0.00224024 
0.000359127 
8.24275e-05 
1.5221e-05 
0.00239839 
0.000925833 
8.38452e-06 
5.00242e-05 
0 
8.15635e-05 
0.00144068 
0.000458594 
1.86325e-05 
8.72197e-05 
2.02836e-05 
0.000124933 
0.00203024 
0.000799149 
0.00123346 
0.00149695 
0.000435343 
0.000718392 
3.35989e-05 
0.00182854 
0.00262812 
2.76479e-05 
0.00250575 
0.0020803 
0.00153801 
1.68824e-05 
0.000455287 
6.18795e-05 
0.000173711 
1.36817e-05 
0.000249775 
3.48058e-05 
0.00137565 
0.000279592 
0.000252187 
0.000242169 
0.000499853 
3.71132e-05 
1.73137e-05 
2.63224e-05 
9.6156e-06 
3.69368e-06 
1.1282e-06 
0.000485775 
0.00090759 
0.000162374 
0.000808344 
0.00141227 
0.000232301 
0.000703273 
0.00127551 
0.000550431 
0.000137509 
6.03868e-05 
0.000393238 
0.000600832 
6.44911e-06 
0.000321068 
0.0020399 
0.00010829 
0.00106271 
0.000627269 
0.000490388 
0.00062107 
0.00465119 
0.00117575 
0.000382935 
0.00375391 
0.00326469 
0.001188 
0.000384069 
0.000761689 
0.00012355 
0.000480219 
0.000187271 
0.000340116 
4.71508e-05 
0.000158864 
0 
0.0014427 
0.00355725 
3.62268e-06 
0 
0.00160566 
0.000105647 
0.000196056 
0.00881145 
0.000130275 
2.60452e-05 
0.000201501 
0.000254535 
0.000700214 
0.000268487 
0.000260433 
0.000197329 
0.000146447 
0.000547699 
0.00029899 
5.59151e-05 
0.000112466 
0.000142163 
0.00478472 
0.00033166 
0.00111648 
0.000701067 
0.0005184 
0.000104413 
0.000797266 
0.000220656 
7.1329e-05 
8.03534e-05 
4.36992e-05 
4.12865e-06 
1.37261e-05 
4.20488e-05 
1.89874e-05 
4.46605e-05 
5.0543e-05 
5.18044e-05 
0.000452892 
0.000126146 
6.6525e-05 
0.000122974 
8.36469e-07 
0.000190946 
0.000242847 
0.000974992 
0.000685473 
5.37947e-05 
4.48067e-05 
1.14192e-05 
8.26861e-05 
3.17868e-06 
8.51808e-06 
9.42572e-05 
0.000261808 
0.000526752 
9.90134e-05 
0.000100907 
3.72576e-07 
3.98175e-05 
0.000274153 
7.83499e-05 
0.000217636 
5.49963e-05 
0.00273305 
0.00935611 
0.000369439 
4.49656e-05 
5.34798e-06 
8.59155e-05 
0.000102739 
4.99053e-05 
0.000129942 
1.36316e-05 
0.00014979 
0.000383086 
3.51949e-07 
0.00541202 
3.32144e-05 
1.50035e-05 
0.000214426 
0.000104484 
2.94687e-05 
4.44505e-07 
6.24643e-05 
0.000265589 
9.63237e-06 
5.09e-05 
2.92337e-05 
4.05238e-05 
1.16074e-05 
0.00010184 
8.8024e-05 
0 
3.45088e-05 
0.000133881 
1.94677e-05 
4.09566e-08 
2.80491e-05 
0.000176312 
0 
0.000424553 
0.00037083 
0.000266381 
0.000818559 
5.03251e-05 
0.00298073 
0.00471534 
5.69515e-06 
8.67279e-05 
1.63581e-06 
0.00024595 
5.18774e-05 
0.000160653 
3.59053e-05 
7.58783e-05 
6.65786e-05 
2.30019e-05 
4.57522e-05 
7.38785e-06 
5.61901e-06 
0 
2.34782e-05 
0.00048007 
6.49645e-05 
0.000295987 
0.000149092 
6.17717e-05 
0.000128778 
3.74029e-06 
0.000267924 
5.76562e-05 
5.40096e-05 
0.000150814 
0.000159841 
0.000784711 
4.96949e-05 
0.00014803 
0.000132201 
0.000195492 
3.69929e-05 
3.2279e-06 
3.87749e-06 
1.42055e-05 
0.000682904 
3.6881e-06 
3.38282e-06 
1.6296e-06 
0.000111801 
0.0001029 
1.48758e-05 
0 
5.41473e-05 
5.02567e-06 
0.000280656 
1.05827e-06 
1.42209e-05 
6.98463e-05 
4.47983e-05 
0.000480893 
5.19507e-05 
1.49566e-06 
0.000167392 
3.76887e-05 
0 
1.24411e-06 
1.27962e-05 
0.000147646 
3.30637e-05 
2.53505e-05 
1.50673e-05 
9.68425e-06 
5.17947e-06 
3.35793e-06 
6.36904e-07 
0.000476826 
7.60934e-05 
0.000150566 
0.000229235 
0.000165658 
6.36904e-07 
7.3942e-05 
3.80856e-05 
5.55432e-05 
8.66291e-05 
9.63929e-07 
1.40579e-05 
1.19464e-06 
1.93555e-05 
4.3956e-06 
8.71719e-06 
1.54233e-05 
1.39013e-05 
1.9635e-05 
6.29753e-07 
7.19417e-05 
3.58812e-05 
1.38578e-06 
6.61027e-05 
4.28438e-06 
1.46624e-06 
3.38297e-05 
3.35347e-06 
0.000143006 
3.27747e-05 
2.34678e-05 
5.52432e-06 
5.06632e-05 
0.000233478 
1.96942e-06 
7.0069e-07 
1.57031e-05 
0 
3.09132e-05 
2.5486e-06 
5.38302e-07 
0 
2.36102e-06 
4.32968e-05 
4.62467e-07 
8.34973e-07 
0 
1.85897e-05 
5.03926e-07 
0 
4.12267e-05 
2.06976e-05 
1.88934e-07 
0 
1.22878e-05 
9.62484e-05 
3.69146e-05 
9.70573e-05 
0.000142559 
9.79091e-05 
9.76446e-06 
1.54756e-06 
7.96789e-07 
3.48624e-05 
0.000120669 
2.15273e-05 
2.2197e-06 
1.61359e-06 
1.91556e-06 
9.57694e-07 
5.87915e-08 
1.48289e-06 
4.10912e-05 
2.53718e-05 
1.62558e-05 
6.38894e-06 
4.56217e-06 
5.27775e-06 
0.000817181 
4.3336e-05 
7.34821e-05 
1.25256e-06 
1.19072e-05 
0.000102759 
6.52889e-05 
8.0071e-07 
2.54084e-05 
8.74302e-07 
1.41872e-06 
5.52605e-07 
5.83388e-07 
5.7228e-06 
7.32719e-06 
8.83263e-07 
1.52881e-05 
4.23381e-06 
5.00362e-06 
1.76848e-05 
6.00348e-06 
1.40919e-06 
4.3e-06 
9.63034e-06 
2.11865e-06 
0 
4.76908e-08 
4.76908e-08 
3.92909e-07 
6.3993e-05 
9.47603e-07 
0.00269393 
0.000776777 
1.27867e-06 
2.45979e-06 
4.14742e-05 
2.52537e-05 
4.08494e-05 
2.43279e-05 
4.4304e-07 
6.36904e-07 
6.16285e-07 
3.68502e-07 
4.82819e-07 
2.63583e-05 
3.18277e-05 
3.10859e-06 
1.39248e-05 
7.60925e-06 
1.58621e-07 
6.32979e-07 
2.27872e-05 
1.35745e-06 
7.97598e-05 
2.07021e-05 
1.11265e-06 
1.18649e-06 
5.71483e-05 
1.90443e-08 
1.27731e-07 
6.32979e-07 
0 
9.20875e-08 
2.33629e-06 
9.64665e-06 
3.38282e-06 
1.11616e-05 
1.16473e-06 
9.7741e-06 
3.57688e-06 
3.36787e-06 
0 
2.13218e-06 
3.31924e-07 
5.63876e-07 
4.53642e-07 
1.21737e-06 
5.65241e-06 
0 
1.19108e-06 
0.000614601 
3.52373e-06 
1.40289e-05 
3.46136e-07 
1.34243e-07 
2.90234e-08 
2.90234e-08 
7.7497e-06 
5.00145e-06 
6.87652e-07 
6.31285e-07 
1.93237e-05 
2.69654e-07 
2.64572e-08 
2.82126e-06 
0 
1.4263e-07 
2.2635e-05 
1.56344e-06 
1.33836e-05 
1.03466e-06 
4.94072e-07 
1.57841e-07 
8.71551e-07 
1.47229e-05 
2.86205e-07 
6.32979e-07 
0 
1.56626e-06 
2.50324e-05 
3.52005e-07 
1.4818e-06 
9.6543e-07 
2.0298e-07 
1.24233e-07 
1.4189e-05 
1.67669e-06 
2.63623e-07 
1.04479e-06 
2.6101e-07 
1.54361e-08 
0 
1.57822e-07 
1.24233e-07 
1.24233e-07 
0 
3.54724e-06 
5.65241e-06 
3.6164e-06 
1.23259e-07 
4.09603e-07 
0 
0 
2.19802e-05 
8.05961e-07 
1.41872e-06 
9.44629e-07 
0 
4.33926e-05 
1.77048e-06 
2.86205e-07 
1.77048e-06 
1.14482e-06 
4.57928e-06 
6.11203e-08 
2.65792e-06 
1.04024e-06 
1.79302e-08 
3.54724e-06 
0 
1.31297e-06 
8.9498e-05 
0 
0 
2.86205e-07 
0 
0 
6.32979e-07 
0 
6.32979e-07 
0 
2.26988e-07 
1.88262e-08 
0 
1.56607e-07 
3.9159e-07 
0 
1.07364e-07 
7.34275e-08 
3.94904e-05 
0 
0 
6.32979e-07 
1.24233e-07 
0 
0 
4.3428e-05 
6.73649e-08 
2.53589e-06 
0 
1.1218e-08 
0 
2.13255e-08 
3.78045e-08 
7.29903e-08 
0 
# Learning phase...
 - Before training
Accuracy: 7184/9950 = 0.72201
Average error toward 0: -0.00018406 (1818)
Average error toward 1: 7.649e-05 (948)
Prev: 0.442513 (error: 0.182714) Offset: 0 0
Ratio error 1 : 0.342733
-----------------
- 3729 - 948 - 
-----------------
- 1818 - 3455 - 
-----------------
 - Phase 1
Accuracy: 8892/9950 = 0.893668
Average error toward 0: -0.000195 (538)
Average error toward 1: 0.000086 (520)
Prev: 0.442513 (error: 0.054070) Offset: 0.000000 0.000000
Ratio error 1 : 0.491493
-----------------
- 5009 - 520 - 
-----------------
- 538 - 3883 - 
-----------------
 - Phase 2
Accuracy: 9175/9950 = 0.922111
Average error toward 0: -0.000197 (393)
Average error toward 1: 0.000088 (382)
Prev: 0.442513 (error: 0.039497) Offset: 0.000000 0.000000
Ratio error 1 : 0.492903
-----------------
- 5154 - 382 - 
-----------------
- 393 - 4021 - 
-----------------
 - Phase 3
Accuracy: 9244/9950 = 0.929045
Average error toward 0: -0.000198 (354)
Average error toward 1: 0.000089 (352)
Prev: 0.442513 (error: 0.035578) Offset: 0.000000 0.000000
Ratio error 1 : 0.498584
-----------------
- 5193 - 352 - 
-----------------
- 354 - 4051 - 
-----------------
 - Phase 4
Accuracy: 9259/9950 = 0.930553
Average error toward 0: -0.000198 (344)
Average error toward 1: 0.000089 (347)
Prev: 0.442513 (error: 0.034573) Offset: 0.000000 0.000000
Ratio error 1 : 0.502171
-----------------
- 5203 - 347 - 
-----------------
- 344 - 4056 - 
-----------------
 - Phase 5
Accuracy: 9285/9950 = 0.933166
Average error toward 0: -0.000199 (331)
Average error toward 1: 0.000090 (334)
Prev: 0.442513 (error: 0.033266) Offset: 0.000000 0.000000
Ratio error 1 : 0.502256
-----------------
- 5216 - 334 - 
-----------------
- 331 - 4069 - 
-----------------
 - Verification
Accuracy: 9321/9950 = 0.936784
Average error toward 0: -0.000199 (297)
Average error toward 1: 0.000090 (332)
Prev: 0.442513 (error: 0.029849) Offset: 0.000000 0.000000
Ratio error 1 : 0.527822
-----------------
- 5250 - 332 - 
-----------------
- 297 - 4071 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 748 0.983957
0.935864
Accuracy: 1069/1104 = 0.968297
Average error toward 0: -0.000000 (15)
Average error toward 1: 0.000000 (20)
Prev: 0.442513 (error: 0.001508) Offset: 0.000000 0.000000
Ratio error 1 : 0.571429
-----------------
- 595 - 20 - 
-----------------
- 15 - 474 - 
-----------------
# Prediction serialization...
# Saving the [1104,738] weight matrix...
4.582531
