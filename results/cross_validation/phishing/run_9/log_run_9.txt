# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,735] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
0.00207188 
0.0541675 
0.00108876 
0.00091902 
0.00827373 
0.00317193 
0.0116467 
0.070336 
0.00682018 
0.000117336 
0.000931681 
0.00144581 
0.00389687 
0.00847345 
0.00101364 
0.000302273 
0.000716322 
0.0323684 
0.000443342 
0.00164467 
0.00412108 
0.000249374 
0.00155111 
0.0229382 
0.0253794 
0.00230079 
0.000221467 
0.00586991 
0.00413119 
0.00510292 
0.00808012 
0.00288256 
0.0087126 
0.00258263 
0.00262584 
0.000688471 
0.000782725 
0.00304505 
0.00400164 
0.000201258 
0.00101059 
0.00498047 
0.000274197 
0.0430106 
0.0061681 
0.00213342 
0.00168343 
0.0180679 
0.0103401 
0.00481858 
0 
0.00693152 
0.00949746 
0.0082518 
0.00923532 
0.00241168 
0.0061526 
0.00145073 
0.00063682 
0.00557247 
0.000219592 
0 
0.0627608 
0.000441971 
0.00141111 
0.0111902 
0.00150114 
0.00103803 
9.19413e-05 
4.27274e-06 
0.0107435 
0.00224311 
0.00823509 
0.0191899 
0.00155371 
0.00229861 
0.000818879 
1.2114e-05 
1.26136e-05 
0.000598289 
0.000473519 
3.3928e-06 
0.000835187 
6.47414e-05 
0.00207318 
0.00711764 
0.00269198 
6.55192e-05 
0.00457238 
4.80114e-06 
0.000250214 
0.00174437 
2.02763e-07 
0.00146399 
0.0090479 
0.00110727 
0.00800456 
0.00291936 
0.00421396 
0.00655597 
0.00507209 
0.000140611 
0.000295689 
0.0146348 
0.000129764 
0.000117067 
0.00340549 
0.00111418 
0.000690557 
0.00142619 
0.000486828 
0.000110371 
0.0035195 
0.000120694 
0.000195664 
2.54611e-05 
0.00126545 
4.26957e-05 
0.00127494 
0.00013755 
0.000166291 
0.0024436 
0.00238529 
0.000856014 
0.000380269 
0.000287329 
0.0017589 
0.000125501 
0.000639372 
5.14314e-05 
0.00193922 
0.000355431 
0.00128438 
0.00031813 
6.50897e-05 
0.000811002 
0.0002472 
0.000237816 
0.000999414 
0 
0.000392856 
0.000371538 
0.000355088 
0.000221855 
0.000469003 
0.00129524 
0.000300003 
8.05507e-05 
0.000228952 
0.00572385 
0.00359877 
0.000921679 
0.000845054 
0.00220635 
5.98742e-05 
0.00726544 
0.000525911 
0.00243537 
0.00307656 
0.000663261 
0.000104156 
0.00267814 
0.0012843 
0.00445339 
0.00316554 
0.000590329 
0.000117566 
0.0136202 
0 
0.0019477 
4.34044e-05 
0.000173604 
0.000698691 
0.00359535 
0.000680159 
0.000350328 
0.000424867 
0.00138289 
0.00120337 
0.000468991 
0.000497251 
0.00213366 
0.000275907 
0.00376165 
0.0101378 
0.00282832 
0.00943395 
0.000713625 
4.92966e-05 
0.0029998 
0.00050136 
8.04318e-05 
6.38317e-07 
0.000764161 
0.00113051 
5.05851e-05 
3.44765e-06 
0.00189545 
0.000390158 
0.000684439 
0.000518685 
4.72846e-05 
0.00014193 
0.000350432 
0.000122709 
0.0009676 
9.3436e-05 
2.6635e-06 
0.00019823 
0.000273802 
0.00135756 
0.000657389 
0.000300825 
0.00255403 
0.00131701 
0.000680012 
0.000357315 
0.000291594 
4.08644e-05 
0.00162547 
2.06068e-05 
0.000907224 
0.000520453 
0.000358155 
0.00249796 
0.00190483 
6.21947e-06 
2.94413e-05 
0.000477793 
0.000287768 
0.000931566 
0.00331764 
0.00314844 
0.000198831 
0.000249619 
0.00238254 
0.000629171 
0.000391287 
0.000211463 
0.000291462 
0.000728264 
0.00342711 
0.000396381 
0.00862838 
0.00584349 
0.00275493 
0.000190188 
0.000272326 
9.21366e-05 
0.000562498 
3.9049e-05 
0.000714729 
0.000136643 
0.000940903 
0.000300408 
0.000373095 
3.39922e-05 
0.000279111 
0.000135753 
0.000141725 
6.26071e-05 
2.15305e-05 
0.000491065 
5.36916e-05 
0.000251712 
0.00177923 
0.000810231 
0.000735587 
0.000647813 
0.000119954 
5.03927e-05 
0.00371086 
8.28525e-05 
0.000523582 
0.00120236 
0.000178909 
0.000845426 
0.0006922 
0.00245336 
0.00204116 
1.62774e-05 
1.65282e-05 
2.54445e-05 
4.07783e-05 
0.0088011 
0.00984159 
0.00145503 
0.000485247 
0.00011997 
0.000150788 
4.83203e-06 
2.10785e-05 
0.000284948 
0.000296941 
0.000821699 
6.93873e-05 
3.05904e-05 
8.83511e-05 
5.84151e-06 
0.000231762 
9.19683e-05 
0.000408789 
5.37718e-05 
8.33571e-05 
0.00071247 
0.00211406 
0.000427618 
0.00906088 
0.000711868 
9.25005e-05 
0.000143334 
2.88012e-06 
0.000134739 
0.00143064 
0.00482355 
0.00167164 
4.83616e-05 
0.000385285 
0.000130368 
3.83574e-05 
0.000264111 
5.34578e-05 
1.83716e-06 
2.18624e-05 
0.000542432 
5.72606e-06 
1.34283e-05 
5.66028e-05 
1.35495e-05 
9.15669e-05 
0.000273638 
0.000272888 
0.00175852 
3.93049e-05 
1.90893e-05 
0.000124652 
0.000246898 
1.22505e-06 
0.000191864 
0.000371057 
1.79995e-05 
0.000242284 
0.000276355 
1.73178e-06 
0.000380248 
9.99125e-06 
5.37775e-05 
6.60461e-05 
4.1113e-05 
9.29748e-06 
0.00363309 
2.44042e-05 
0.000127047 
6.82763e-05 
5.51455e-05 
2.00276e-05 
0.000374682 
0.000467183 
0.00019257 
0.000402984 
0.000545494 
0.000211255 
0.000105222 
2.88739e-07 
0.00186645 
0.000136471 
9.87502e-05 
0.0017028 
0.000212156 
4.86938e-06 
0.00013961 
1.01351e-05 
5.00194e-05 
0.000719602 
0.0156666 
0.00289031 
0.000502557 
0.000162302 
3.04169e-05 
0.000491539 
0.00824102 
7.69527e-05 
4.21892e-05 
0 
5.86839e-05 
2.36179e-05 
8.31674e-06 
7.79353e-05 
8.54522e-05 
4.90629e-06 
0.000885702 
0.000864594 
0.000153087 
1.49635e-06 
4.77494e-05 
0 
5.12361e-05 
5.69094e-05 
3.59866e-06 
0.000654077 
0.000199359 
8.84504e-06 
7.66955e-05 
1.58038e-05 
0.00136596 
0.00918232 
0.00481194 
0.00318761 
0.000105049 
0.00244887 
0 
0.000162185 
0.000461705 
0.000366131 
3.73749e-07 
2.97056e-05 
0.000161742 
0.000129647 
0.000110728 
4.50477e-05 
1.66078e-05 
4.39361e-05 
6.06548e-07 
0.000466109 
0.000242482 
2.89292e-06 
0.000133721 
1.07803e-05 
1.52637e-05 
7.03299e-06 
9.31e-06 
3.71932e-05 
1.33708e-07 
0.000110394 
9.60806e-06 
6.8546e-05 
0 
0.00135952 
3.14266e-05 
5.09714e-05 
2.24137e-05 
8.6982e-06 
5.34746e-05 
0.00016977 
1.25235e-06 
1.36535e-05 
2.0181e-05 
2.1839e-05 
1.19403e-05 
8.43486e-06 
4.34126e-06 
6.41078e-05 
2.36939e-06 
5.49083e-06 
0 
3.52568e-05 
4.62201e-05 
1.83325e-05 
3.61357e-07 
8.70898e-05 
3.61357e-07 
4.48436e-05 
5.91398e-06 
5.28628e-05 
0.000106993 
4.12535e-05 
6.79016e-05 
1.84805e-05 
4.29357e-05 
8.80327e-06 
1.59886e-05 
1.63198e-06 
1.90805e-06 
1.49074e-05 
3.35005e-07 
3.13887e-06 
1.79861e-06 
5.99717e-06 
2.1139e-05 
1.30786e-06 
9.58414e-05 
1.16291e-06 
1.20363e-05 
5.9561e-05 
0.0028876 
0.00484131 
4.31623e-05 
2.88166e-08 
2.2143e-05 
4.06633e-05 
1.13366e-05 
1.53062e-06 
0.000922722 
0.0005097 
5.62057e-05 
1.2649e-06 
2.05821e-05 
1.45921e-06 
0.000306471 
2.33821e-05 
3.11455e-06 
2.91989e-05 
3.83731e-06 
7.43651e-06 
9.1737e-06 
0 
1.12358e-06 
0.000105165 
6.51473e-05 
3.16683e-06 
0.000134875 
0.000271587 
8.65155e-05 
5.42996e-05 
0.000109468 
2.27313e-05 
3.59349e-06 
0.000242241 
7.30373e-05 
6.57376e-05 
5.58344e-05 
0.000161981 
0.000808854 
0.00016126 
3.5145e-05 
1.26972e-06 
0 
4.51629e-06 
1.04052e-05 
2.87752e-05 
2.67902e-05 
0.000114096 
8.59949e-06 
6.90232e-07 
4.82062e-05 
9.27419e-06 
6.95658e-07 
2.15921e-06 
3.87758e-05 
1.91569e-05 
2.49178e-05 
5.42927e-06 
2.60334e-06 
1.78829e-05 
8.20547e-05 
1.94527e-05 
4.53361e-07 
2.73865e-06 
1.90294e-05 
5.5189e-06 
3.87417e-05 
5.55889e-06 
0 
3.01563e-05 
0 
8.70183e-06 
1.93239e-05 
6.10603e-06 
0.000165739 
7.43711e-06 
1.4683e-06 
4.06425e-08 
1.04203e-05 
1.41267e-07 
9.99132e-07 
4.28729e-05 
1.99781e-05 
1.56736e-06 
0.000212831 
1.34375e-05 
1.04021e-06 
0.000123729 
7.01354e-07 
2.48063e-05 
1.58587e-07 
0 
6.38402e-07 
1.21339e-06 
3.6336e-05 
3.66104e-06 
1.24106e-05 
0 
0 
1.24154e-05 
3.61357e-07 
6.16405e-07 
7.57162e-06 
2.41427e-05 
2.15849e-06 
2.88166e-08 
2.9355e-05 
1.10277e-05 
4.6931e-08 
4.75846e-07 
1.93001e-06 
1.12902e-06 
2.63616e-05 
4.98698e-07 
1.60227e-06 
4.07627e-08 
0.000147323 
0.000129821 
3.2917e-05 
2.05578e-07 
1.67003e-07 
6.16906e-05 
3.91401e-06 
5.72149e-07 
1.34663e-06 
4.45464e-07 
8.16531e-07 
5.55889e-06 
1.46907e-05 
1.18912e-06 
1.17425e-06 
2.63175e-07 
2.37004e-05 
1.00138e-07 
5.67828e-07 
5.42508e-07 
4.55379e-05 
2.01661e-06 
1.5487e-08 
8.76107e-07 
1.79971e-06 
1.43206e-06 
1.44333e-05 
0 
3.68058e-05 
6.24953e-07 
3.28951e-06 
6.04574e-07 
3.21336e-07 
3.95218e-06 
2.76711e-07 
5.6375e-07 
8.94786e-07 
1.96459e-06 
0 
2.92118e-06 
1.74249e-06 
9.31e-06 
2.9819e-06 
4.71403e-07 
2.28247e-08 
5.85158e-08 
0 
1.22888e-07 
4.87005e-08 
0 
3.08206e-06 
4.87005e-08 
0 
2.16937e-05 
3.02497e-06 
8.05285e-07 
2.04249e-05 
2.83106e-07 
7.88223e-06 
3.69498e-07 
9.47652e-07 
2.83106e-07 
1.16847e-05 
1.01918e-05 
6.04552e-07 
4.10165e-07 
3.60892e-06 
1.00496e-06 
4.25374e-07 
0 
4.51765e-05 
2.58008e-06 
1.7828e-06 
1.7828e-06 
1.03203e-05 
2.76711e-07 
2.83106e-07 
6.4502e-07 
0 
6.23687e-08 
2.16508e-08 
3.48333e-07 
3.69498e-07 
3.10471e-06 
1.61083e-06 
7.48067e-08 
2.29759e-06 
2.92118e-06 
0 
0 
5.44628e-07 
1.32963e-06 
0.000856158 
8.06755e-05 
0 
7.3649e-07 
0 
6.4502e-07 
0 
2.46952e-06 
0 
2.83106e-07 
0 
0 
2.83106e-07 
0 
8.93476e-07 
1.888e-08 
0 
7.05595e-07 
0 
2.44076e-07 
7.3298e-08 
4.11109e-05 
0 
2.83106e-07 
8.18811e-07 
2.28247e-08 
0 
0 
4.43722e-05 
6.89099e-08 
2.83106e-07 
0 
4.41059e-06 
0 
1.12004e-08 
0 
2.14203e-08 
3.85246e-08 
7.29921e-08 
6.4502e-07 
0 
4.87005e-08 
1.56504e-06 
# Learning phase...
 - Before training
Accuracy: 7180/9950 = 0.721608
Average error toward 0: -0.000182246 (1757)
Average error toward 1: 8.2844e-05 (1013)
Prev: 0.441508 (error: 0.176583) Offset: 0 0
Ratio error 1 : 0.365704
-----------------
- 3800 - 1013 - 
-----------------
- 1757 - 3380 - 
-----------------
 - Phase 1
Accuracy: 8908/9950 = 0.895276
Average error toward 0: -0.000195 (534)
Average error toward 1: 0.000092 (508)
Prev: 0.441508 (error: 0.053668) Offset: 0.000000 0.000000
Ratio error 1 : 0.487524
-----------------
- 5023 - 508 - 
-----------------
- 534 - 3885 - 
-----------------
 - Phase 2
Accuracy: 9155/9950 = 0.920101
Average error toward 0: -0.000196 (402)
Average error toward 1: 0.000094 (393)
Prev: 0.441508 (error: 0.040402) Offset: 0.000000 0.000000
Ratio error 1 : 0.494340
-----------------
- 5155 - 393 - 
-----------------
- 402 - 4000 - 
-----------------
 - Phase 3
Accuracy: 9229/9950 = 0.927538
Average error toward 0: -0.000197 (361)
Average error toward 1: 0.000094 (360)
Prev: 0.441508 (error: 0.036281) Offset: 0.000000 0.000000
Ratio error 1 : 0.499307
-----------------
- 5196 - 360 - 
-----------------
- 361 - 4033 - 
-----------------
 - Phase 4
Accuracy: 9266/9950 = 0.931256
Average error toward 0: -0.000198 (341)
Average error toward 1: 0.000095 (343)
Prev: 0.441508 (error: 0.034271) Offset: 0.000000 0.000000
Ratio error 1 : 0.501462
-----------------
- 5216 - 343 - 
-----------------
- 341 - 4050 - 
-----------------
 - Phase 5
Accuracy: 9241/9950 = 0.928744
Average error toward 0: -0.000198 (354)
Average error toward 1: 0.000095 (355)
Prev: 0.441508 (error: 0.035578) Offset: 0.000000 0.000000
Ratio error 1 : 0.500705
-----------------
- 5203 - 355 - 
-----------------
- 354 - 4038 - 
-----------------
 - Verification
Accuracy: 9329/9950 = 0.937588
Average error toward 0: -0.000199 (304)
Average error toward 1: 0.000095 (317)
Prev: 0.441508 (error: 0.030553) Offset: 0.000000 0.000000
Ratio error 1 : 0.510467
-----------------
- 5253 - 317 - 
-----------------
- 304 - 4076 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 733 0.974079
0.908921
Accuracy: 1054/1104 = 0.954710
Average error toward 0: -0.000000 (17)
Average error toward 1: 0.000000 (33)
Prev: 0.441508 (error: 0.001709) Offset: 0.000000 0.000000
Ratio error 1 : 0.660000
-----------------
- 582 - 33 - 
-----------------
- 17 - 472 - 
-----------------
# Prediction serialization...
# Saving the [1104,735] weight matrix...
4.637977
