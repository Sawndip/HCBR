# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,735] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
0.00211031 
0.0320363 
4.10593e-05 
0.0621463 
0.000127828 
0.0133973 
0.00369786 
5.2131e-05 
0.00722563 
5.74625e-05 
0.00121735 
0.00324857 
1.84419e-07 
0.00412576 
0.0148856 
0.0183631 
0.000751393 
5.85034e-05 
0.0189957 
0.0691147 
0.00103309 
0.0106286 
0.0445386 
0.0258477 
2.25433e-08 
0.00023998 
0.0110642 
4.72401e-05 
0.000661746 
0.000969482 
0.00142438 
2.98995e-05 
0.00177461 
0.00822189 
0.00840565 
0.000752058 
0.00516302 
0.00432876 
0.00777316 
0.00151211 
0.00229855 
0.0020578 
0.00257496 
0.00906523 
0.000301893 
0.00694237 
0.0096736 
0.00361105 
0.000198992 
0.00425265 
0.00115153 
0.000119175 
0.00448642 
0.00294943 
0.00203252 
0.0055361 
0.00142381 
0.00356829 
0.00679918 
0.0115202 
0.00891198 
0.001346 
0.000461279 
7.17408e-05 
0.00735218 
0 
0.0529836 
7.379e-05 
0.00486523 
0.000131838 
0.0232223 
0.00401866 
0.00111749 
0.00308824 
4.1285e-05 
0.0065728 
0.00144563 
0.0104546 
0.000345131 
0.000472642 
0.005685 
0.00485301 
0.00209169 
0.000301256 
0.000748257 
0.00862682 
0.00205511 
0.00232411 
0.00125503 
0.00583743 
0.00883345 
0.00114059 
0.000233939 
8.74928e-05 
0.000287667 
0.000247994 
0.000565641 
0.000178889 
0.000491481 
0 
0.00858053 
0.000428184 
0.000932813 
0.00164412 
0.00627518 
1.82828e-05 
0.000447849 
0.000423446 
0.00134501 
5.47075e-05 
0.000172487 
0.0012489 
0.00825799 
0.00262977 
5.76756e-05 
0.000352435 
0.00341618 
0.00261509 
0.00241633 
0.000737597 
0.000258169 
0.00274033 
0.00310258 
0.000369616 
0.00924623 
0.000841291 
4.37724e-05 
2.71561e-05 
0.000716746 
0.000478443 
0.00216232 
0.000880041 
0.00267362 
0.00844004 
0.00903078 
0.000683951 
0.000513398 
0.00257401 
0.00142083 
0.00131857 
0.000667852 
0.000350774 
0.00810675 
0.00239323 
0.00296868 
0.000214836 
4.67074e-05 
0.000374363 
0.00170049 
8.12436e-07 
0.00147541 
0.000634649 
0.000659164 
0.000277706 
0.00489555 
0.000543075 
0.000739523 
0.000493478 
0.00285188 
0.000252798 
0.00346045 
4.29319e-05 
0.00183713 
0.00488485 
0.00123521 
0.00143449 
0.000113265 
0.00061562 
0.00097691 
0.000757529 
0.0011355 
0.000881821 
2.346e-05 
0.00175879 
0.000801868 
0.000970986 
0.00040507 
0 
0.00193739 
0.000907359 
0.000320212 
0.000533635 
0.00408449 
0.00418687 
0.00245507 
0.00123817 
0.00013899 
0.00528593 
8.81672e-05 
0.00036323 
4.17533e-05 
0.000271813 
4.9658e-05 
0.00140755 
0.00119411 
0.00305798 
0.000390638 
0.000344338 
0.00229938 
0.00246103 
0.0054446 
0.00137433 
0.000611363 
1.29311e-06 
0.000282782 
3.54713e-05 
1.78442e-05 
3.71567e-05 
3.29995e-05 
0.000252577 
6.82524e-07 
0.000643257 
2.20757e-05 
1.18005e-05 
0.000456695 
0.000110619 
0.00043344 
0.00312688 
0.00102142 
0.000253833 
0.00181912 
0.00260802 
0.000893387 
0.000303781 
0.000216141 
0.00582295 
0.000256809 
0.000519819 
0.000392139 
0.000729311 
0.000135298 
0.000489682 
7.87732e-05 
0.000819727 
0.000702223 
5.01601e-05 
4.64122e-05 
0.000244927 
0.00172791 
0 
7.43173e-05 
0.000134857 
0.000270338 
0.000273019 
6.26937e-05 
0.000245316 
0.000223736 
0.000463813 
0.000347605 
0.000978847 
0.000124223 
0.000186654 
0.000297242 
4.20315e-06 
0.00173516 
0.00190084 
0.00070017 
0.00104532 
0.00156444 
0.000100874 
0.00308347 
0.00266644 
0.00831782 
0 
0.000122546 
0.000230673 
0.00157177 
6.57021e-05 
0.000222471 
9.71878e-05 
1.79032e-05 
8.82851e-05 
0.000140455 
0.000673455 
6.08168e-05 
2.4063e-06 
0.000387901 
0.00021696 
3.45351e-05 
0.00279891 
0.00175385 
0.000557852 
0.00217206 
0.00246496 
0.00243887 
0.00186542 
0.000235506 
0.000289092 
9.81156e-05 
0.000815997 
5.99702e-07 
0.00967613 
1.57756e-05 
0.0101048 
2.21071e-06 
0.00157979 
8.46048e-05 
0.000115902 
1.84487e-05 
8.4771e-05 
0.000291068 
0.000197826 
0.000164044 
0.000471304 
2.27864e-05 
0.000656092 
0.00489836 
0.000173635 
7.04601e-07 
0.00150645 
5.18233e-05 
1.04787e-06 
5.41303e-06 
0.000192396 
0.000687153 
1.96855e-05 
0.000232877 
0.000376486 
8.43841e-05 
4.27868e-05 
3.16556e-05 
7.08192e-05 
1.18353e-05 
1.60981e-05 
0.00183772 
0.000114231 
0.000701983 
0.000359047 
0.000478251 
0.000122519 
0 
1.20149e-05 
0.000128181 
0.000372925 
2.35065e-05 
3.74748e-05 
6.0672e-05 
4.98723e-05 
0.000137627 
0.000731697 
0.00031452 
0.0101062 
2.62154e-05 
0.000551112 
0.000249569 
0.00237917 
3.64881e-07 
1.60229e-05 
0.000391915 
0.000298561 
0.000822993 
4.32495e-05 
6.22598e-06 
1.03003e-05 
3.08413e-06 
1.90309e-07 
2.32259e-05 
4.80777e-08 
3.12381e-05 
1.39645e-05 
0.00032359 
0.000214239 
9.32416e-06 
8.83439e-06 
7.72225e-06 
0.000350776 
5.86783e-06 
7.21535e-05 
8.99996e-05 
0.00355756 
0.000151518 
0.000444758 
5.63634e-05 
0.000385242 
5.50667e-06 
0.00369415 
2.99694e-06 
0.00028859 
0.000678192 
5.32476e-05 
3.89799e-05 
1.91995e-05 
7.69121e-05 
0.000794072 
1.16723e-05 
6.03553e-05 
4.80661e-05 
0.000322404 
2.40091e-06 
6.41875e-05 
2.17169e-06 
2.60238e-05 
1.01212e-05 
0.000115807 
4.4496e-05 
9.32392e-05 
0.000123786 
0.000540972 
6.46884e-05 
0.000170206 
0.000793396 
3.63445e-05 
0.00035632 
8.70858e-07 
0.00129023 
0.000644475 
1.6558e-05 
6.0338e-05 
6.45452e-06 
0.000161114 
8.13979e-07 
6.83533e-05 
3.75466e-05 
3.77603e-06 
8.80445e-07 
2.67286e-05 
0.000124176 
1.20549e-05 
8.47897e-07 
4.00139e-05 
4.59928e-06 
2.25881e-06 
6.65955e-05 
2.27603e-05 
1.53183e-06 
2.40056e-05 
8.90193e-05 
1.03929e-05 
9.49241e-06 
5.90512e-05 
2.25192e-06 
3.02632e-05 
1.21619e-06 
4.99246e-05 
5.87015e-05 
3.67885e-06 
2.49378e-05 
0.000207826 
1.82202e-06 
1.24612e-05 
2.10029e-05 
0.000345918 
9.70421e-06 
7.32946e-05 
9.16056e-05 
7.59188e-05 
0.000161107 
4.57743e-05 
4.36315e-05 
0.00310131 
0.000127778 
3.55922e-05 
0.000182038 
4.06685e-05 
0.00023681 
0.000134496 
4.64112e-06 
0.000265981 
4.01625e-06 
3.43791e-07 
0.00015863 
0.000770679 
5.48245e-05 
1.02711e-05 
9.59149e-06 
0.000496934 
2.78784e-05 
1.66677e-05 
9.20542e-06 
4.39348e-07 
0.000140905 
9.50878e-06 
0.000140831 
1.68126e-05 
7.66075e-06 
8.09898e-05 
1.51722e-05 
2.17036e-05 
7.43227e-08 
0.000121152 
6.56698e-06 
0 
3.24004e-05 
4.50878e-05 
7.39871e-05 
1.72421e-06 
0.000123894 
7.61634e-05 
1.39423e-06 
2.25433e-08 
1.86878e-07 
0 
8.09523e-07 
6.32572e-07 
1.11079e-06 
0 
0.00280169 
0.000616419 
2.96516e-05 
3.03783e-05 
2.18651e-06 
8.60546e-06 
4.75598e-06 
2.1364e-06 
2.05329e-06 
1.51393e-06 
6.47507e-05 
0 
8.60206e-05 
7.89372e-05 
9.04618e-05 
0.0159175 
1.95887e-05 
4.80703e-05 
8.5368e-06 
0.000276415 
0.000168812 
0.000117646 
6.22697e-05 
2.19498e-06 
8.18597e-05 
1.39789e-06 
9.23651e-05 
2.35749e-05 
4.09815e-06 
3.07187e-06 
3.55395e-06 
2.70168e-06 
0.000942962 
1.5103e-06 
2.16035e-05 
4.74226e-06 
6.20638e-06 
1.58284e-05 
9.94097e-05 
4.94984e-05 
1.00974e-05 
0.000725007 
4.28106e-05 
7.11088e-06 
0 
0.000133851 
0.000138464 
2.96137e-05 
1.5472e-05 
8.123e-06 
1.91514e-05 
5.40662e-05 
8.22929e-06 
3.00444e-05 
5.60839e-07 
9.86537e-07 
0 
3.50911e-05 
1.21064e-07 
0 
1.49825e-06 
6.93156e-07 
1.24025e-05 
1.88028e-06 
5.57445e-07 
3.25092e-06 
2.73851e-07 
3.607e-05 
1.51229e-05 
0 
0.000115486 
0.000269175 
3.67686e-07 
2.38812e-07 
1.45336e-06 
2.1337e-06 
5.51262e-05 
1.59014e-06 
7.61124e-06 
4.45107e-07 
4.65562e-07 
7.7438e-05 
2.96666e-06 
5.28627e-06 
0 
0.000102904 
1.58773e-05 
5.06805e-05 
0 
0 
0.00491614 
0.000116063 
1.28346e-06 
0.00014891 
5.50049e-06 
3.17588e-07 
5.98221e-07 
3.98991e-05 
1.14581e-06 
5.5012e-05 
2.83594e-08 
2.50175e-05 
1.94901e-06 
4.03074e-05 
0.000155487 
8.03202e-07 
9.20542e-06 
9.89131e-08 
2.67465e-06 
0.000560101 
5.76515e-08 
2.73851e-07 
1.90309e-07 
3.8775e-07 
9.38712e-07 
0.000498997 
0.000212515 
2.12781e-06 
0.000111125 
4.21704e-06 
4.53386e-05 
4.57321e-05 
6.0205e-07 
0.000146085 
4.99662e-07 
1.50193e-05 
5.50049e-06 
1.04319e-06 
4.81012e-06 
0 
1.25877e-06 
1.05231e-05 
1.84908e-05 
3.56702e-07 
4.47152e-07 
8.49559e-06 
1.76657e-06 
1.3953e-06 
6.20904e-07 
1.39789e-06 
5.37368e-07 
0 
8.28541e-07 
4.07184e-08 
3.56702e-07 
0 
1.51659e-05 
1.36503e-05 
3.06473e-06 
7.88166e-05 
6.17139e-07 
4.11479e-06 
2.42537e-06 
0.000216618 
0 
8.30709e-07 
0 
1.12433e-06 
4.637e-06 
9.4355e-07 
1.39768e-07 
0.000805187 
1.24597e-06 
3.62673e-06 
7.96008e-07 
2.51901e-05 
6.23272e-07 
0 
0.000830441 
2.39636e-05 
2.12437e-05 
3.56702e-07 
4.0474e-07 
3.42671e-07 
1.40488e-06 
1.61125e-07 
2.51169e-06 
6.23272e-07 
2.24378e-05 
1.11494e-06 
6.32572e-07 
6.23272e-07 
0 
9.05325e-08 
1.13957e-05 
4.37962e-06 
3.86678e-06 
0 
1.48448e-06 
1.32666e-07 
2.83594e-08 
5.93141e-06 
1.21201e-06 
1.13283e-05 
1.04235e-06 
6.91668e-07 
1.56045e-07 
2.53029e-06 
0 
2.51169e-06 
6.23272e-07 
0 
2.83772e-05 
1.82404e-05 
2.62614e-06 
2.63723e-07 
1.53921e-08 
0 
0 
4.56011e-06 
2.30072e-07 
0 
2.74504e-05 
9.38768e-07 
4.61725e-05 
6.02155e-08 
4.56011e-06 
0 
1.30949e-06 
8.96503e-05 
0 
6.32572e-07 
0 
0 
0 
6.23272e-07 
0 
0 
6.23272e-07 
1.87877e-08 
1.55358e-07 
3.87616e-07 
0 
1.81994e-08 
0 
4.27583e-05 
6.62883e-08 
0 
1.10843e-08 
# Learning phase...
 - Before training
Accuracy: 7200/9950 = 0.723618
Average error toward 0: -0.000183462 (1782)
Average error toward 1: 7.51687e-05 (968)
Prev: 0.44392 (error: 0.179095) Offset: 0 0
Ratio error 1 : 0.352
-----------------
- 3751 - 968 - 
-----------------
- 1782 - 3449 - 
-----------------
 - Phase 1
Accuracy: 8897/9950 = 0.894171
Average error toward 0: -0.000195 (539)
Average error toward 1: 0.000085 (514)
Prev: 0.443920 (error: 0.054171) Offset: 0.000000 0.000000
Ratio error 1 : 0.488129
-----------------
- 4994 - 514 - 
-----------------
- 539 - 3903 - 
-----------------
 - Phase 2
Accuracy: 9190/9950 = 0.923618
Average error toward 0: -0.000197 (384)
Average error toward 1: 0.000086 (376)
Prev: 0.443920 (error: 0.038593) Offset: 0.000000 0.000000
Ratio error 1 : 0.494737
-----------------
- 5149 - 376 - 
-----------------
- 384 - 4041 - 
-----------------
 - Phase 3
Accuracy: 9230/9950 = 0.927638
Average error toward 0: -0.000198 (360)
Average error toward 1: 0.000087 (360)
Prev: 0.443920 (error: 0.036181) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 5173 - 360 - 
-----------------
- 360 - 4057 - 
-----------------
 - Phase 4
Accuracy: 9240/9950 = 0.928643
Average error toward 0: -0.000198 (355)
Average error toward 1: 0.000088 (355)
Prev: 0.443920 (error: 0.035678) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 5178 - 355 - 
-----------------
- 355 - 4062 - 
-----------------
 - Phase 5
Accuracy: 9260/9950 = 0.930653
Average error toward 0: -0.000199 (344)
Average error toward 1: 0.000088 (346)
Prev: 0.443920 (error: 0.034573) Offset: 0.000000 0.000000
Ratio error 1 : 0.501449
-----------------
- 5189 - 346 - 
-----------------
- 344 - 4071 - 
-----------------
 - Verification
Accuracy: 9293/9950 = 0.933970
Average error toward 0: -0.000199 (348)
Average error toward 1: 0.000088 (309)
Prev: 0.443920 (error: 0.034975) Offset: 0.000000 0.000000
Ratio error 1 : 0.470320
-----------------
- 5185 - 309 - 
-----------------
- 348 - 4108 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 769 0.977893
0.922585
Accuracy: 1062/1104 = 0.961957
Average error toward 0: -0.000000 (19)
Average error toward 1: 0.000000 (23)
Prev: 0.443920 (error: 0.001910) Offset: 0.000000 0.000000
Ratio error 1 : 0.547619
-----------------
- 604 - 23 - 
-----------------
- 19 - 458 - 
-----------------
# Prediction serialization...
# Saving the [1104,735] weight matrix...
4.552014
