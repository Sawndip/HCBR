# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,736] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
0.000792708 
0.0155341 
1.22366e-06 
0.00874027 
0.0708584 
0.00914414 
8.32715e-05 
0.000717796 
0.0630053 
0 
0.00301968 
0.000709672 
0.00686299 
0.00269851 
0.00153182 
0.00119277 
0.0196554 
0.00783988 
0.000842895 
0.000219495 
0.00230539 
0.00247394 
3.686e-06 
0.0115118 
0.0537593 
8.77588e-05 
9.35021e-05 
0.00930234 
0.00164502 
0.000291708 
8.43772e-05 
0.0070143 
0.00834512 
0.0179774 
0.0110573 
0 
0.00489078 
0.00358661 
0.00359349 
0.03205 
0.0109142 
0.0019153 
0.00103758 
0.00313096 
0.000237457 
0 
0.00130696 
0.00114239 
0.000715326 
0.000120016 
0.00366919 
0.00437282 
0.000329743 
0.00331523 
0.00835487 
0.000820827 
2.34428e-05 
0.000109324 
0.000273275 
0.00389479 
0.000419212 
0.00147796 
0.00244146 
6.89584e-05 
0.00195962 
0.0166285 
0.000301915 
0.00456855 
0.000388355 
0.00155129 
0.0226114 
0.00841492 
0.00371507 
0.00217807 
0.000378908 
0.00172684 
0.00217783 
0.00107533 
5.85075e-06 
0.0018024 
0.00195882 
0.00854478 
0.00507103 
0.00738708 
0.000177239 
0.000287629 
0.000332475 
9.89738e-05 
0.00774165 
0.043362 
0.00469377 
0.000391294 
0.000841272 
0.0133699 
0.000576662 
0.00200465 
0.00112294 
0.00262792 
0.00298859 
0.000940522 
7.98462e-05 
0.00210878 
0.00280422 
0.00450854 
0.000943762 
7.0077e-05 
0.0103241 
0.00295859 
0.0029727 
0.000132591 
0.000140062 
0.00154956 
0.00976089 
1.67252e-05 
2.7546e-05 
0.000483589 
2.84679e-07 
0.00133667 
0.00202451 
0.00574628 
0.00180816 
0.00279692 
0.000313569 
0.000814577 
0.00877199 
0.00743035 
0.00956648 
0.000479764 
0.00124737 
0.000227653 
0.0084751 
0.000390211 
0.000701406 
0 
0.0102491 
0.00125859 
0.00136107 
0.000612528 
0.000277109 
0.000821207 
0.000560285 
0.000649738 
0.00211954 
0.000719569 
0.00139515 
0.00495269 
0.000675867 
0.000432293 
0.000101928 
0.000201539 
0.00380304 
0.0060881 
0.0015849 
0.000840809 
0.00275523 
0.00406796 
0.0033441 
0.00893254 
0.00140334 
4.44644e-05 
0.000381747 
0.000213139 
0.00045934 
0.000240259 
0.000790695 
0.00071466 
0.00185663 
1.34914e-05 
0.0005225 
0.0253876 
0.000195748 
0.000123932 
0.00148225 
3.36908e-06 
0.00155577 
0.000786802 
0.00237265 
0.00245958 
0.00115683 
0.00012764 
0.000186697 
0.000912797 
0.00106551 
0.000205746 
4.44101e-05 
0.00316071 
0.000707654 
0.000124008 
0.000222506 
3.95623e-05 
0.000648422 
0.000156144 
0.00235908 
2.08224e-05 
1.54308e-05 
0.000104549 
0 
0.00239881 
0.000198316 
0.000238353 
0.000362011 
0.00416476 
0.00633243 
0.00323597 
5.57187e-05 
0.00138824 
0.00239909 
0.00252666 
0.000298823 
0.00246961 
0.000853424 
0.00131321 
0.00212098 
0.00164781 
0.000606785 
5.99416e-05 
0.00536195 
0.00100036 
1.69898e-05 
7.02731e-05 
0.000731204 
0.000321828 
0.00141663 
0.00163482 
0.000734839 
4.13207e-05 
0.00101347 
9.70018e-06 
0.000127308 
0.000225557 
0.000368917 
0.000289541 
0.000160968 
0.000188525 
8.06037e-06 
0.000662825 
5.5753e-05 
0.000283331 
0.00372179 
0.000666066 
0.000683645 
0.0018288 
0.000948999 
0.00120607 
1.02505e-05 
0.000513218 
0.000494541 
0.000231596 
0.00069475 
1.40541e-05 
1.9455e-05 
0.000856073 
0.000370726 
0.000530114 
0.00490811 
0.000263387 
0.000262264 
0.00196671 
0.000145243 
0.00018354 
0.000527558 
0.00109095 
2.18927e-05 
0.00016193 
9.42936e-08 
0.000272015 
0.000245849 
0.00311181 
0.000145241 
0.00010745 
4.55227e-05 
2.49491e-05 
3.95004e-05 
0.000301171 
1.12764e-05 
6.3554e-05 
0.000350326 
0.00553613 
0.00538191 
0.000503705 
0.000273021 
0.000515048 
0.000380829 
0.000622511 
0.000874578 
6.2008e-05 
5.12911e-07 
3.64417e-05 
0.000521653 
3.82744e-05 
0.000124302 
1.79891e-06 
0.000122909 
4.56815e-05 
7.09258e-05 
1.18663e-05 
0.00212701 
0.000481183 
0.0026807 
0.000274457 
0.00474461 
0.000228505 
0.00164616 
3.69724e-06 
3.84241e-05 
0.000526907 
0.000797378 
0.000530093 
7.10989e-06 
0.000786458 
2.62965e-05 
5.54864e-06 
0.000152812 
0.000277312 
0 
0.000112556 
1.12567e-06 
0.00243941 
0.000155053 
0.000230419 
3.86576e-05 
0.000366674 
0.00339076 
0.00106103 
0.00025311 
0.00113888 
0.000364146 
3.31486e-05 
0.000147969 
0.000295965 
4.79875e-05 
9.53264e-05 
2.24958e-05 
2.33427e-05 
5.60508e-05 
0.00125674 
3.29078e-06 
5.06657e-05 
0.00033721 
9.9168e-05 
0.000393447 
7.69469e-05 
0.00415823 
5.41772e-05 
5.98579e-05 
0.000235558 
0.000506366 
0.000102607 
3.24213e-05 
8.57789e-05 
0.00078194 
0.000314817 
0.000493477 
0.000771162 
2.86409e-06 
0.000176244 
0.000112679 
0.000319845 
0.000178998 
0.000567429 
8.56852e-05 
0.000148505 
0.000715721 
0.000117657 
0.00030266 
0.000249578 
0.000414497 
0.000242612 
1.452e-05 
3.55276e-05 
5.95741e-05 
4.91949e-06 
1.17789e-06 
4.09986e-05 
2.89758e-05 
1.92589e-07 
1.95754e-05 
0.000543077 
1.92589e-07 
4.68811e-06 
4.36682e-05 
0.000652612 
4.70686e-05 
4.80117e-05 
0.00140991 
0.000410461 
0.000707695 
0.000152659 
0.000364164 
6.17386e-05 
0.000360918 
0.0001034 
0.000139657 
8.80242e-05 
9.60738e-06 
8.60962e-06 
5.40728e-06 
0.000169941 
4.52226e-05 
1.7863e-06 
0.000204346 
2.1251e-05 
0.00014589 
5.83223e-05 
0.0101252 
8.88854e-05 
0.000126818 
4.54388e-05 
6.55561e-05 
9.30321e-07 
1.26914e-06 
4.8339e-05 
2.06108e-06 
7.7627e-07 
8.45831e-05 
2.12991e-05 
0.000258134 
4.91365e-05 
0.00023305 
3.7188e-05 
0.000158801 
0.000119538 
6.35081e-05 
4.84916e-05 
5.59883e-05 
1.60164e-05 
5.49518e-05 
2.70574e-05 
1.01332e-06 
1.79102e-05 
1.24722e-05 
0 
4.83825e-05 
2.12849e-05 
2.28754e-05 
1.63555e-05 
9.77181e-06 
1.61153e-05 
7.05778e-05 
7.23268e-05 
5.30677e-05 
3.3355e-05 
1.20312e-06 
0.000114626 
0.000353801 
0.000133084 
0.000236255 
2.31984e-05 
1.95392e-05 
2.38955e-06 
2.18158e-05 
1.1612e-05 
0.00012461 
6.4346e-05 
1.92897e-05 
4.1571e-05 
1.95663e-06 
1.85974e-06 
2.00351e-05 
1.6491e-06 
1.12086e-05 
0 
0.00875403 
0.00563359 
3.50616e-05 
2.72279e-06 
4.35638e-07 
6.39296e-05 
4.30432e-06 
1.49403e-05 
8.78054e-05 
6.86466e-07 
6.58681e-05 
3.43499e-05 
0.00079845 
9.11559e-05 
4.76199e-05 
3.2947e-07 
0.000192983 
6.73397e-05 
6.65769e-06 
9.03909e-05 
6.69167e-05 
6.0553e-06 
0.000143654 
4.83799e-05 
6.42472e-07 
6.64768e-05 
4.98609e-05 
0.000130803 
2.63517e-05 
6.42472e-07 
7.86072e-05 
5.44488e-06 
0.00039405 
1.12644e-06 
0.000155667 
4.89937e-05 
3.11169e-05 
1.07357e-05 
7.36168e-06 
9.5354e-05 
8.5918e-07 
7.6971e-05 
3.90673e-06 
1.24797e-06 
8.44847e-07 
1.88579e-07 
0 
0 
2.96743e-05 
6.07108e-07 
0 
4.73687e-05 
8.04476e-05 
0.000147824 
3.1056e-05 
0.00472638 
0.00304842 
0.000217022 
1.09958e-06 
0.00276901 
3.48616e-07 
1.77702e-06 
3.41567e-05 
0.000123069 
4.8764e-06 
8.97192e-06 
7.06953e-06 
7.05026e-05 
4.96789e-06 
0.000154077 
4.47058e-05 
1.15001e-06 
2.63216e-05 
1.46706e-05 
5.04105e-07 
1.51928e-05 
2.21e-06 
0.00887556 
0.00120647 
5.80384e-05 
9.36169e-05 
9.70684e-06 
1.74425e-06 
4.40012e-06 
0 
0 
2.29555e-05 
2.07693e-05 
2.20893e-06 
2.90169e-08 
2.97383e-06 
2.90169e-08 
1.01485e-05 
5.6131e-06 
2.78037e-06 
1.35548e-05 
0 
4.74565e-05 
3.83218e-05 
4.43654e-05 
4.76563e-06 
9.07878e-06 
0 
2.28712e-08 
6.32201e-06 
0 
2.68903e-06 
3.22308e-07 
5.56986e-06 
2.45955e-05 
7.35038e-06 
3.12989e-06 
3.92356e-06 
1.20688e-05 
2.91589e-05 
8.02369e-06 
2.39778e-06 
3.73153e-05 
8.41253e-06 
1.34168e-06 
0.00046309 
0.000232634 
4.73035e-05 
5.60473e-06 
3.654e-05 
3.57502e-06 
0 
6.24934e-07 
1.28401e-06 
5.59667e-07 
8.04061e-06 
5.39794e-06 
1.36467e-05 
6.25971e-07 
4.35581e-05 
1.05893e-06 
4.71571e-07 
4.66725e-06 
1.00548e-06 
0 
5.45009e-07 
3.07063e-05 
2.55484e-07 
0 
1.00331e-06 
2.40999e-05 
6.9036e-08 
9.75638e-07 
1.10615e-06 
2.34576e-06 
1.78571e-05 
1.83691e-05 
5.56986e-06 
1.43001e-05 
1.78697e-06 
4.47187e-06 
1.03226e-05 
0 
1.0166e-06 
4.99149e-06 
2.07112e-05 
1.35715e-05 
2.35544e-05 
8.59288e-06 
9.86087e-07 
7.12044e-07 
2.32319e-07 
2.84679e-07 
0 
1.00143e-05 
1.76663e-06 
9.50819e-07 
1.34097e-07 
3.65301e-05 
1.81404e-05 
1.24529e-05 
1.80548e-05 
0 
8.25189e-07 
3.1265e-06 
0.000461097 
5.90519e-07 
2.28712e-08 
3.29787e-06 
6.45161e-07 
0 
7.89404e-06 
4.529e-07 
5.67359e-07 
2.45059e-06 
1.2871e-06 
0.000114292 
4.87419e-06 
1.9069e-06 
3.93002e-07 
7.96768e-07 
0 
2.7777e-07 
1.22562e-06 
1.42661e-06 
1.0533e-06 
4.51208e-07 
0 
0 
3.72596e-06 
0.000593 
5.88124e-08 
1.4161e-07 
2.01101e-07 
9.81824e-07 
4.46309e-07 
9.33225e-06 
9.33225e-06 
1.23279e-07 
1.1415e-08 
1.44735e-06 
0 
0 
1.8491e-08 
2.16751e-08 
3.84689e-08 
7.43621e-08 
6.45161e-07 
5.44673e-07 
0 
0 
8.21424e-07 
4.88235e-08 
2.2671e-07 
3.01115e-06 
7.50211e-08 
0 
1.86841e-06 
2.7777e-07 
5.65655e-07 
2.78744e-08 
1.00236e-07 
2.60498e-06 
1.44735e-06 
6.59637e-05 
0.000236957 
0 
3.64645e-06 
0.000833671 
1.74721e-06 
8.22606e-07 
6.42472e-07 
1.61378e-07 
2.84679e-07 
1.78697e-06 
2.84679e-07 
1.02485e-05 
9.34457e-08 
0 
1.61129e-07 
0 
2.58064e-06 
2.84679e-07 
0 
2.65124e-07 
1.57952e-08 
0 
2.19901e-05 
4.45074e-05 
6.26427e-08 
3.57502e-06 
0 
0 
6.45161e-07 
0 
0 
2.84679e-07 
0 
1.91577e-08 
# Learning phase...
 - Before training
Accuracy: 7193/9950 = 0.722915
Average error toward 0: -0.000186852 (1808)
Average error toward 1: 7.51495e-05 (949)
Prev: 0.440302 (error: 0.181709) Offset: 0 0
Ratio error 1 : 0.344215
-----------------
- 3761 - 949 - 
-----------------
- 1808 - 3432 - 
-----------------
 - Phase 1
Accuracy: 8920/9950 = 0.896482
Average error toward 0: -0.000198 (526)
Average error toward 1: 0.000085 (504)
Prev: 0.440302 (error: 0.052864) Offset: 0.000000 0.000000
Ratio error 1 : 0.489320
-----------------
- 5043 - 504 - 
-----------------
- 526 - 3877 - 
-----------------
 - Phase 2
Accuracy: 9166/9950 = 0.921206
Average error toward 0: -0.000200 (395)
Average error toward 1: 0.000086 (389)
Prev: 0.440302 (error: 0.039698) Offset: 0.000000 0.000000
Ratio error 1 : 0.496173
-----------------
- 5174 - 389 - 
-----------------
- 395 - 3992 - 
-----------------
 - Phase 3
Accuracy: 9212/9950 = 0.925829
Average error toward 0: -0.000201 (369)
Average error toward 1: 0.000087 (369)
Prev: 0.440302 (error: 0.037085) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 5200 - 369 - 
-----------------
- 369 - 4012 - 
-----------------
 - Phase 4
Accuracy: 9247/9950 = 0.929347
Average error toward 0: -0.000202 (351)
Average error toward 1: 0.000088 (352)
Prev: 0.440302 (error: 0.035276) Offset: 0.000000 0.000000
Ratio error 1 : 0.500711
-----------------
- 5218 - 352 - 
-----------------
- 351 - 4029 - 
-----------------
 - Phase 5
Accuracy: 9265/9950 = 0.931156
Average error toward 0: -0.000202 (344)
Average error toward 1: 0.000088 (341)
Prev: 0.440302 (error: 0.034573) Offset: 0.000000 0.000000
Ratio error 1 : 0.497810
-----------------
- 5225 - 341 - 
-----------------
- 344 - 4040 - 
-----------------
 - Verification
Accuracy: 9284/9950 = 0.933065
Average error toward 0: -0.000202 (386)
Average error toward 1: 0.000088 (280)
Prev: 0.440302 (error: 0.038794) Offset: 0.000000 0.000000
Ratio error 1 : 0.420420
-----------------
- 5183 - 280 - 
-----------------
- 386 - 4101 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 780 0.979487
0.936428
Accuracy: 1069/1104 = 0.968297
Average error toward 0: -0.000000 (12)
Average error toward 1: 0.000000 (23)
Prev: 0.440302 (error: 0.001206) Offset: 0.000000 0.000000
Ratio error 1 : 0.657143
-----------------
- 576 - 23 - 
-----------------
- 12 - 493 - 
-----------------
# Prediction serialization...
# Saving the [1104,736] weight matrix...
4.584442
