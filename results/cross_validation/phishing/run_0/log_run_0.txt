# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,737] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
0 
0.00670694 
8.15979e-05 
0.000696816 
0.000155585 
0.0053801 
0.00167709 
0.00248192 
0.000110577 
0.052616 
1.88882e-05 
0.00635934 
0.0686497 
0.000362744 
0.00221072 
0.00893968 
0.00904219 
0.0109614 
0.00155136 
0.032045 
0.00223382 
0.0187196 
0.0432629 
0.004941 
0.00513772 
0.0259289 
0.000870646 
0.00168508 
0.00845005 
0.00360734 
0.0138539 
0.00593905 
0.0113897 
0.00422357 
0.00324563 
0.00436928 
0.00578006 
0.0621936 
0.000723327 
0.0102335 
0.0039147 
0.00851322 
0.00363566 
0.000248879 
0 
0.00740568 
0.000270754 
0.0101735 
0.00817883 
0.00730996 
0.000144639 
0.00310784 
0.00032977 
0.000250178 
0.00120338 
0.000815714 
0.000305878 
0.00672564 
0.0235311 
0.00821331 
0.00372907 
0.00282923 
0.00145639 
0.000263273 
0.0027303 
0.0194846 
0.0100561 
0.000356225 
0.0148137 
0.000695338 
0.000281164 
0.00138135 
0.00205241 
0.00673871 
0.00228903 
0.000919936 
0.0106952 
0.000411219 
0.000918144 
0.00069999 
2.83709e-05 
0.00069786 
0.00349243 
0.00144094 
0.00122988 
0.000412993 
0.00075878 
1.2542e-05 
0.00560408 
0.000848619 
0.0014988 
0.000298666 
0.000109739 
0.00799155 
0.000153283 
0.00132409 
0.00224889 
0.00152504 
0.0001002 
0.000125304 
0.00113169 
0.00316643 
0.00184679 
0.000784953 
0.000241621 
0.00466058 
0.00289364 
0.00146643 
4.51527e-06 
0.000743596 
0.00133057 
0.000820588 
1.96882e-05 
1.91108e-05 
0.00907388 
1.25248e-05 
0.000472734 
0.00156062 
0.00310093 
0.000545154 
3.57817e-07 
0.00310461 
0.00522598 
0.00116089 
5.47203e-05 
0.000230001 
0.00175291 
0.000369016 
0.000679282 
0.00108038 
7.39346e-05 
6.55993e-05 
4.19029e-05 
0.000490057 
0.00167015 
0.00434177 
0.0020897 
0.00346645 
0.00095122 
0.000846223 
0.000280437 
0.000141709 
0.00471502 
0.00100618 
0.00390082 
0.000190773 
0 
0.000298833 
0.00265472 
0.00229497 
0.000477048 
0.00139534 
4.52564e-05 
0.00034165 
0.00121518 
0.0084815 
0.00387546 
0.000389345 
5.36074e-05 
0.000594958 
0.00070918 
0.000325714 
0.00051772 
0.00283837 
0.00129078 
0.00167695 
0.000870791 
0.00236189 
0.000672438 
0.00196111 
0.0029815 
0.000813142 
0.00186444 
0.00100283 
5.05599e-05 
0.000350041 
0.00161973 
0.000313772 
6.9844e-05 
0.000427145 
0.000330266 
0.00339978 
0.000141105 
0.00109333 
0.00199996 
0.00250459 
0.000254483 
0.00164582 
8.8662e-05 
9.58232e-06 
0.000353302 
0.000211545 
0.00246523 
0.000212448 
0.00230232 
0.000984819 
2.80244e-05 
0.000105799 
0.00049906 
0.000369804 
6.24701e-06 
0.000713962 
0.000271152 
2.18046e-06 
0.000231263 
0.00128702 
0.00142529 
0.000411255 
5.20166e-05 
0.000187592 
5.33263e-05 
0.000185454 
0.000518046 
0.00073191 
8.7072e-05 
0.000988994 
0.000146045 
0.000123567 
0.000180591 
6.76005e-06 
1.19404e-05 
0.000516948 
5.82175e-05 
5.00082e-05 
7.34891e-07 
0.00478416 
0.00356754 
0.00207823 
3.68536e-06 
0.000157196 
2.14858e-05 
0.000107751 
0.000895011 
0.0027474 
0.000233299 
0.000270159 
0.000274497 
0.000106299 
1.68668e-05 
0.00827426 
0.00126607 
6.37799e-05 
0.000725744 
0.0021644 
0.000631236 
0.000564714 
0.000237318 
0.00011629 
0.00181443 
0.00210066 
4.98419e-05 
0.00068251 
0.00122315 
0.00364782 
0.00190003 
0.00167506 
0.000118575 
0.000182806 
0.000279616 
0.00238502 
0.000278131 
0.00958988 
0.000210811 
0.000130015 
1.3497e-05 
7.46303e-06 
8.51553e-06 
0.00241627 
0.000214271 
0.000367115 
0.000680202 
0.00850162 
0.00398716 
0.00239146 
0.000644052 
0.0024373 
0.000593612 
1.51986e-05 
0.000132945 
0.000133157 
1.2113e-05 
1.5157e-05 
0 
0.00120429 
0.000115798 
0.000265071 
0.000894547 
2.17326e-06 
4.22941e-06 
0.000376107 
2.12432e-05 
0.000518283 
0.00035263 
8.50367e-05 
0.000373795 
0.000102998 
0.00184429 
0.000525405 
0.00114497 
0 
0.000118766 
0.000492694 
0.000118542 
8.4357e-05 
0.00402906 
0.000207864 
0.000772538 
6.57418e-05 
0.00143619 
0.00247822 
1.60592e-05 
0.0103744 
0.000514372 
0.000971155 
0.000747962 
8.47976e-05 
0.000222054 
8.81654e-07 
0.0001794 
7.67897e-05 
2.56262e-05 
0.000301758 
3.19038e-07 
0.000231021 
7.40847e-05 
0.00890048 
4.08973e-05 
0.00586163 
0 
0.000288571 
3.43211e-07 
0.000215076 
1.48531e-05 
6.17006e-05 
0.000516159 
2.27395e-05 
0.000383008 
4.53219e-05 
2.35423e-05 
0.000144744 
3.25116e-06 
0.000470404 
5.57303e-07 
3.64719e-06 
3.71876e-05 
8.99418e-05 
0.000155994 
4.81866e-05 
0.000281323 
3.48546e-05 
0.000257461 
3.59234e-05 
0.000621295 
0.000689098 
0.000271262 
4.06737e-05 
0.000250715 
3.32587e-06 
0.000493575 
0.000134098 
0.00255019 
8.30915e-07 
1.35379e-06 
7.43651e-05 
6.8696e-05 
1.97554e-05 
0.00021018 
0.00111244 
0.000395618 
5.262e-05 
6.55182e-05 
3.87509e-05 
0.000426264 
0.000203162 
7.65569e-05 
6.40701e-05 
5.79918e-06 
0.000133454 
7.04318e-05 
1.06792e-05 
8.5064e-06 
1.79613e-05 
5.53201e-05 
1.0211e-05 
0.000508485 
2.3132e-05 
4.88535e-06 
0.000612688 
1.8696e-06 
2.55619e-05 
0.00489558 
0 
1.52092e-06 
9.44671e-06 
5.17678e-05 
0.00010333 
8.97063e-07 
1.63867e-05 
0.000192408 
0.000195532 
3.75902e-05 
4.29559e-05 
3.12708e-05 
1.0979e-06 
0.000438947 
2.1283e-06 
4.95329e-05 
3.80263e-05 
8.41401e-05 
4.62542e-05 
0.000312691 
3.88483e-05 
0.000241848 
3.04541e-06 
5.83301e-05 
0.000101226 
0.000182715 
5.03256e-05 
9.14206e-07 
4.04767e-08 
0.000128048 
3.98651e-05 
8.08241e-06 
0.000397022 
6.44149e-05 
9.27074e-05 
8.65332e-05 
2.70791e-05 
0.000305824 
1.20629e-06 
3.58796e-07 
0.00014193 
1.16185e-05 
1.27862e-05 
9.47478e-05 
3.70952e-05 
2.05204e-06 
5.9109e-05 
3.26728e-05 
0.00079658 
0.000630742 
0.000369375 
3.63813e-06 
6.81822e-05 
5.57075e-05 
2.53936e-06 
2.6826e-05 
3.74494e-06 
2.54157e-05 
2.11665e-05 
7.54964e-06 
3.37198e-05 
8.06842e-06 
8.15992e-06 
0.000384629 
0.00880346 
0.00862525 
0.000489803 
2.63101e-05 
4.92202e-06 
7.85697e-06 
2.74419e-05 
1.28137e-06 
0 
0.000121155 
0.000154369 
0.015975 
0.000479434 
0.000691772 
1.05901e-05 
2.24477e-05 
2.55107e-05 
2.25283e-08 
1.84892e-06 
0 
6.2762e-05 
2.27627e-05 
3.5948e-06 
5.52089e-05 
1.91521e-05 
0.00489176 
0.00262861 
2.72558e-07 
4.17258e-06 
1.21715e-07 
4.65237e-07 
0 
9.65578e-05 
0.000726121 
0.000133209 
0.000128184 
6.61671e-05 
1.08715e-05 
0.000116682 
5.75129e-05 
1.76872e-06 
6.92862e-05 
1.08848e-05 
5.9865e-07 
3.67e-06 
2.61683e-05 
6.23667e-07 
4.5997e-06 
0 
0.000117296 
8.25648e-05 
3.87021e-06 
1.91164e-07 
0 
8.11104e-07 
0 
3.42231e-05 
3.02028e-05 
5.75082e-05 
6.61696e-07 
3.83722e-05 
6.20284e-05 
1.14507e-06 
2.70175e-05 
3.89302e-05 
2.86213e-05 
4.46742e-06 
2.35643e-08 
1.05362e-05 
6.95475e-06 
7.58835e-06 
5.60583e-07 
5.56275e-06 
1.6033e-05 
1.43448e-05 
0.000533835 
0.000391878 
0.0028831 
5.21592e-05 
3.04516e-05 
2.112e-06 
3.57657e-06 
0 
3.79917e-06 
2.18079e-06 
5.78697e-08 
7.99983e-07 
1.61604e-05 
9.04396e-07 
1.08457e-05 
7.88183e-05 
2.56081e-05 
0 
2.6632e-05 
0.000462307 
5.3288e-05 
5.05581e-05 
1.82148e-05 
1.15004e-05 
2.25314e-05 
0.000172826 
4.58092e-06 
9.46604e-07 
5.99924e-06 
3.66158e-07 
1.87719e-05 
4.09158e-05 
1.31759e-06 
1.84845e-06 
2.22308e-06 
1.14382e-05 
4.42014e-07 
3.71884e-06 
8.02617e-07 
8.33088e-05 
4.59534e-05 
9.37948e-07 
0 
4.82992e-06 
2.25202e-06 
0.000212953 
0.000127858 
5.37941e-05 
4.30723e-05 
4.65537e-06 
1.83237e-05 
2.25355e-05 
4.27509e-08 
1.73657e-06 
4.17818e-05 
4.23817e-05 
0.00013844 
6.77423e-05 
7.39178e-06 
0.000174253 
0 
1.93346e-05 
3.43347e-07 
4.08507e-07 
8.84664e-06 
0.000130967 
2.91036e-05 
0.000148156 
1.51543e-06 
9.08439e-07 
9.7557e-05 
2.79749e-06 
1.4331e-05 
3.11109e-05 
3.95972e-08 
1.79543e-06 
6.08509e-07 
6.23656e-06 
9.92427e-07 
6.91085e-07 
8.39424e-06 
4.43389e-05 
2.58732e-07 
0 
0 
5.49546e-07 
1.48098e-07 
1.19302e-06 
1.86947e-07 
0 
1.87782e-05 
5.62587e-06 
2.29079e-05 
3.50258e-05 
1.29253e-06 
8.19616e-07 
1.91164e-07 
3.66724e-05 
1.17279e-05 
2.52471e-06 
4.41107e-06 
2.29092e-05 
1.52647e-05 
5.32618e-06 
5.13455e-07 
2.72558e-07 
6.25987e-07 
6.36368e-07 
0 
6.1109e-08 
0 
0.000115742 
9.14901e-08 
4.82655e-07 
8.02617e-07 
0 
1.1078e-06 
1.26898e-06 
2.64717e-06 
1.05558e-06 
3.80621e-05 
2.94504e-06 
7.85697e-06 
4.46742e-06 
1.59221e-06 
7.4197e-08 
2.64105e-06 
3.49979e-08 
4.58092e-06 
0 
8.47679e-07 
0 
6.03726e-07 
7.63731e-06 
0 
1.54563e-06 
5.37734e-07 
3.99739e-07 
1.03137e-06 
6.139e-06 
0.00078138 
8.4787e-05 
3.57817e-07 
0 
1.18373e-06 
1.18736e-06 
6.25987e-07 
6.36368e-07 
0 
2.96212e-06 
6.34132e-06 
0 
2.52471e-06 
6.25987e-07 
0 
0 
6.25987e-07 
0 
8.91444e-07 
0.000486912 
0.000208859 
6.31421e-07 
1.8673e-08 
1.91164e-07 
0 
1.33119e-05 
3.90371e-07 
6.17175e-07 
1.89199e-06 
0 
2.39704e-07 
7.27739e-08 
3.79991e-05 
9.72281e-07 
3.44493e-07 
2.6478e-06 
0 
8.0927e-07 
2.25283e-08 
6.25987e-07 
0 
0 
4.34485e-05 
6.74369e-08 
6.25987e-07 
0 
4.35522e-06 
1.52191e-07 
2.61156e-07 
0 
1.1051e-08 
0 
0 
2.12748e-08 
3.78433e-08 
7.2956e-08 
6.36368e-07 
0 
4.8473e-08 
0 
1.47449e-06 
7.18196e-05 
0.000212919 
1.55867e-07 
1.24412e-06 
3.57817e-07 
3.97522e-08 
# Learning phase...
 - Before training
Accuracy: 7164/9950 = 0.72
Average error toward 0: -0.000179968 (1770)
Average error toward 1: 8.0593e-05 (1016)
Prev: 0.444322 (error: 0.177889) Offset: 0 0
Ratio error 1 : 0.364681
-----------------
- 3759 - 1016 - 
-----------------
- 1770 - 3405 - 
-----------------
 - Phase 1
Accuracy: 8899/9950 = 0.894372
Average error toward 0: -0.000192 (539)
Average error toward 1: 0.000090 (512)
Prev: 0.444322 (error: 0.054171) Offset: 0.000000 0.000000
Ratio error 1 : 0.487155
-----------------
- 4990 - 512 - 
-----------------
- 539 - 3909 - 
-----------------
 - Phase 2
Accuracy: 9183/9950 = 0.922915
Average error toward 0: -0.000194 (384)
Average error toward 1: 0.000092 (383)
Prev: 0.444322 (error: 0.038593) Offset: 0.000000 0.000000
Ratio error 1 : 0.499348
-----------------
- 5145 - 383 - 
-----------------
- 384 - 4038 - 
-----------------
 - Phase 3
Accuracy: 9210/9950 = 0.925628
Average error toward 0: -0.000195 (371)
Average error toward 1: 0.000092 (369)
Prev: 0.444322 (error: 0.037286) Offset: 0.000000 0.000000
Ratio error 1 : 0.498649
-----------------
- 5158 - 369 - 
-----------------
- 371 - 4052 - 
-----------------
 - Phase 4
Accuracy: 9247/9950 = 0.929347
Average error toward 0: -0.000195 (350)
Average error toward 1: 0.000093 (353)
Prev: 0.444322 (error: 0.035176) Offset: 0.000000 0.000000
Ratio error 1 : 0.502134
-----------------
- 5179 - 353 - 
-----------------
- 350 - 4068 - 
-----------------
 - Phase 5
Accuracy: 9249/9950 = 0.929548
Average error toward 0: -0.000195 (350)
Average error toward 1: 0.000093 (351)
Prev: 0.444322 (error: 0.035176) Offset: 0.000000 0.000000
Ratio error 1 : 0.500713
-----------------
- 5179 - 351 - 
-----------------
- 350 - 4070 - 
-----------------
 - Verification
Accuracy: 9323/9950 = 0.936985
Average error toward 0: -0.000196 (376)
Average error toward 1: 0.000093 (251)
Prev: 0.444322 (error: 0.037789) Offset: 0.000000 0.000000
Ratio error 1 : 0.400319
-----------------
- 5153 - 251 - 
-----------------
- 376 - 4170 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 768 0.985677
0.930073
Accuracy: 1066/1104 = 0.965580
Average error toward 0: -0.000000 (23)
Average error toward 1: 0.000000 (15)
Prev: 0.444322 (error: 0.002312) Offset: 0.000000 0.000000
Ratio error 1 : 0.394737
-----------------
- 605 - 15 - 
-----------------
- 23 - 461 - 
-----------------
# Prediction serialization...
# Saving the [1104,737] weight matrix...
4.556746
