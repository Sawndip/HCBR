# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 1
# Verbose level: 0
# Cases: 11055
# Total features: 331650
# Unique features: 814 (ratio: 0.00245439)
# Minimum case size: 30
# Maximum case size: 30
# Average case size: 30
# Add cases...
# Model serialization...
# Saving the [9950,738] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 9950
Serialization of pre-training strength vectors...
0.0014048 
0.0314577 
0.000282422 
0.00226108 
0.00279781 
0.00792595 
0.00348893 
0.0689318 
0.0200475 
0.00041282 
0.00132126 
0.0436813 
6.0821e-08 
0.00851101 
0.0189336 
0.025858 
0.00307115 
0.00113288 
0.00977001 
0.000219144 
0.00392246 
0.0152572 
0.000211906 
0.0108425 
0.0607253 
0.00124883 
0.000666015 
0.00241892 
0.00405578 
0.00129187 
0.000872745 
0.00275056 
0.00560961 
0.0109033 
0.00244906 
0.000271061 
0.000266694 
0.0101339 
0.00818694 
0.00832911 
0.00583417 
0.00110735 
0.00522838 
0.00447184 
0.0116264 
0.00177526 
0.000283341 
0.00547784 
0.00204602 
0.00683892 
0.00154503 
0.00360104 
0.0039172 
0.002064 
0.00423947 
0.00136503 
0.000373203 
0.00206685 
4.90173e-05 
0.0531359 
0.00144226 
7.37982e-05 
0.00295854 
0.00115855 
0.00260814 
0.00182171 
0.000560068 
0.000896592 
0.000953393 
0.000881446 
0.0236305 
0.00706174 
0.000636873 
0.00258223 
2.80609e-06 
0.000279422 
0.00841251 
9.97729e-05 
3.79532e-05 
0.00921503 
0.000254064 
0.000209287 
0.00246562 
1.68523e-05 
0.00322483 
0.000690615 
0.00116404 
0.0047677 
0.00107766 
2.58792e-05 
0.000114843 
0.000130458 
0.00165486 
0.000794306 
0.000482835 
0.00217787 
0.00111341 
0.000239903 
0.00249099 
1.48914e-05 
0.000333862 
0.00168112 
0.00337179 
0.000139084 
0.00149963 
0.00308241 
0.000153641 
0.000308578 
0.00281462 
0.00126675 
0.00322918 
0.000300087 
0.000756248 
0.00248312 
0.00073691 
0.00121903 
0.000123597 
0.00201049 
0.0135278 
0.000653714 
0.000277823 
0.000848454 
0.00150837 
0.00309279 
2.52112e-05 
0.00174162 
0.00948085 
3.06448e-05 
2.18057e-05 
0.000327601 
0.000385549 
0.000827502 
0 
0.00495661 
0.00875659 
0.00449412 
0.000184868 
3.63891e-05 
6.17676e-05 
5.37651e-06 
0.00164404 
0.00629421 
0.0012206 
0.000683649 
0.000164589 
0.00151759 
0.0019404 
0.000148418 
4.67831e-06 
0.000250102 
0.00325835 
0.000909455 
0.000324523 
0.00564799 
0.00359603 
0.00472618 
0.00227948 
0.00172685 
0.000253231 
0.00132264 
0.0012997 
4.16236e-05 
0.00831241 
0.00151384 
0.00018969 
0.00101599 
0.0021139 
0.00343457 
0.000470654 
0.000209286 
0.000569562 
4.13347e-05 
0.00236623 
9.65934e-05 
8.22503e-05 
4.62196e-05 
0.00866329 
0.000394099 
0.00254605 
0.00149477 
0.000743724 
0.00015243 
5.49305e-05 
0.00464595 
0.000209664 
0.000770476 
0.000115101 
0.000220604 
0.000559841 
6.48176e-05 
0.000274912 
0.000143528 
0.00714106 
5.3255e-05 
0.000278385 
0.00337601 
0.00520998 
0.000100352 
4.81599e-05 
0.00014696 
0.000247152 
0.00919429 
0.000274503 
9.93511e-05 
0.00114266 
0.000660377 
0.00177428 
0.00501518 
0.000876946 
6.52062e-06 
0.001787 
0.000143265 
0.000698943 
0.00718289 
1.53394e-06 
0.000127413 
2.11839e-06 
0.00323676 
0.00190556 
0.00252979 
0.0019068 
0.00042341 
0.000102049 
0.00076252 
5.59404e-05 
0.00660192 
0 
0.000631981 
0.000381599 
2.55393e-06 
0.000176462 
0.000382207 
3.60132e-05 
5.00808e-05 
0.000751504 
0.000422147 
6.34761e-06 
0.00044495 
6.97008e-05 
2.28729e-06 
0.000134879 
0.000447661 
0.000534078 
9.58912e-06 
0.00140808 
0.00106233 
0.000151717 
0.00372678 
0.000278303 
1.22049e-06 
0.00425983 
0.00603623 
0.000381437 
0.00857409 
0.000501019 
0.00024572 
0.00106176 
0.00827207 
1.25718e-05 
0 
0.000698307 
0.000120557 
0.00250943 
0.000500399 
0.000470056 
5.33048e-05 
0.000718387 
4.36526e-05 
0.000986119 
0 
0.00351458 
5.78042e-05 
0.000154443 
0.000245049 
6.85472e-05 
0.0103349 
5.35062e-06 
0.000530263 
4.35573e-05 
0.000111342 
0.000782489 
0.000369626 
0.000401497 
0.000651031 
0.00180868 
0.000747143 
0.00152781 
6.14295e-05 
5.41436e-05 
0.000126201 
0.000382924 
0.000142833 
0.000531109 
1.71223e-06 
0.00897589 
0.00999083 
1.91622e-05 
0.000597367 
0.000121091 
0.000863504 
0.000418386 
9.17695e-06 
0.000123741 
0.000498962 
0.00213264 
8.07123e-05 
0.000283365 
0 
5.19301e-06 
2.41136e-06 
0.000102307 
0.000296795 
0.000384693 
5.98206e-07 
4.78329e-05 
4.31008e-05 
0.00232544 
3.6365e-05 
0.000239409 
0.00131556 
0.000343935 
0.00080278 
0.000116367 
0.000133523 
0.000512947 
0.000186702 
0.000438108 
0.00165923 
0.000113622 
3.2357e-05 
0.000688285 
5.74769e-05 
0.000332365 
0.000448593 
0.000242493 
0.000643676 
1.93498e-05 
0.000916669 
0.00027514 
4.55932e-05 
0.000214623 
9.10886e-07 
0.000232841 
0.000209995 
9.70209e-06 
0.000320661 
7.97166e-05 
5.59733e-05 
5.70234e-06 
0.000707739 
2.90415e-05 
0.0090752 
0.000343593 
0.000104531 
2.79048e-05 
1.05972e-05 
6.96275e-05 
0.000111558 
0 
8.93957e-07 
2.86389e-05 
0.00025672 
2.53985e-05 
0.000193367 
1.292e-06 
6.87609e-07 
0.000443335 
0.000211575 
0.000176284 
7.23038e-05 
0.000396278 
0.00237443 
0.000888317 
5.22468e-05 
4.99965e-05 
0.000170368 
0.000984098 
0.000290993 
0.000143177 
2.06765e-05 
0.000747256 
2.72992e-06 
0.000295281 
5.84563e-05 
1.1173e-06 
1.05092e-06 
0.000239223 
8.29295e-07 
4.42154e-05 
0.000282045 
0.00096379 
0.00042736 
1.66518e-05 
4.80806e-07 
5.13925e-05 
3.05263e-05 
6.14537e-06 
1.27036e-05 
0.0163121 
0.000106839 
6.54536e-05 
7.96515e-07 
7.75556e-05 
1.82432e-05 
0.000509589 
0.000216924 
8.33475e-05 
0.000133543 
0.000368304 
0.000134962 
0.0001368 
9.1657e-05 
0.000541733 
5.11751e-05 
1.19385e-05 
6.41308e-05 
0.00016662 
8.80556e-05 
8.80798e-06 
2.55923e-05 
1.22291e-05 
4.06007e-05 
3.76627e-05 
6.97629e-06 
2.52582e-05 
2.50991e-05 
0.000184993 
6.07236e-05 
4.17737e-05 
1.50398e-05 
0.000152925 
8.59923e-06 
0.000129336 
1.32967e-06 
8.74179e-05 
1.16484e-06 
3.69348e-05 
2.37838e-05 
5.62402e-07 
1.27148e-05 
6.08095e-05 
2.43474e-05 
1.74397e-05 
6.55166e-05 
4.24918e-06 
0 
6.92797e-05 
0.000795336 
0.000524179 
0.000134705 
0.0001601 
3.69219e-05 
3.35832e-05 
0.000150201 
8.27251e-05 
3.2898e-05 
0.000439938 
8.90153e-05 
0.000168042 
4.30784e-05 
3.88635e-07 
1.48326e-06 
1.20516e-06 
0 
1.08821e-06 
5.7367e-05 
3.09622e-05 
1.03924e-05 
1.40373e-05 
5.23555e-05 
2.79164e-05 
9.80337e-05 
1.40285e-05 
5.91237e-05 
1.83557e-05 
7.07988e-05 
3.21282e-05 
1.86571e-05 
7.24296e-05 
1.63228e-05 
2.73891e-05 
7.92269e-06 
0.000348801 
1.8854e-07 
0.000154586 
0 
5.33522e-06 
2.85854e-08 
0.000197517 
0.000122472 
7.28621e-05 
0.000704422 
2.14428e-05 
2.79635e-06 
4.608e-06 
9.8501e-06 
2.85854e-08 
7.23473e-06 
5.44296e-05 
9.58021e-07 
4.11234e-08 
0.00504688 
0.00268503 
4.47473e-05 
6.62668e-06 
2.63455e-05 
0 
7.71936e-07 
7.42457e-05 
3.28105e-07 
4.82008e-05 
1.12853e-05 
6.26915e-07 
2.41252e-05 
1.55016e-06 
9.45747e-06 
2.74608e-06 
8.50612e-06 
1.91181e-06 
1.48698e-06 
3.0671e-07 
1.18539e-05 
5.74579e-06 
4.99213e-05 
2.21184e-05 
0.00266922 
0.000605481 
0.000159778 
2.12674e-05 
4.6164e-06 
5.5547e-07 
0 
1.0805e-05 
2.37407e-06 
3.26045e-06 
3.85733e-06 
0 
1.09817e-06 
2.73186e-06 
0.0029575 
1.58434e-06 
2.76366e-06 
0.000485824 
7.42266e-08 
2.41993e-05 
7.23243e-06 
3.09748e-06 
1.59698e-05 
1.38016e-05 
7.60523e-05 
6.72985e-05 
3.30846e-05 
1.22498e-06 
8.73736e-07 
6.64657e-05 
3.79505e-06 
2.29698e-07 
2.5127e-06 
4.78922e-06 
9.20642e-06 
1.18287e-05 
2.29698e-07 
1.59717e-05 
1.89803e-06 
1.04833e-05 
1.39838e-07 
3.65272e-07 
1.9879e-05 
1.37383e-05 
9.05631e-08 
0 
9.29487e-05 
3.57988e-05 
5.25206e-05 
2.48409e-05 
2.65895e-06 
3.40643e-06 
1.92383e-07 
0 
1.21234e-07 
8.53678e-07 
2.35444e-08 
4.19524e-05 
2.24166e-05 
5.99561e-07 
1.05563e-05 
1.28828e-05 
0 
5.06791e-06 
2.70324e-05 
2.51082e-06 
1.78425e-05 
1.07867e-05 
2.24644e-08 
2.11535e-06 
1.21234e-07 
1.89007e-06 
4.64939e-07 
0 
8.20025e-07 
3.42452e-06 
4.27671e-06 
2.15421e-06 
5.36254e-07 
3.20901e-05 
4.74779e-05 
3.93818e-07 
2.63104e-06 
5.29396e-05 
2.2231e-06 
1.86838e-06 
9.11165e-06 
1.33289e-05 
2.22435e-06 
5.10684e-07 
0.000847735 
8.47486e-05 
6.26915e-07 
5.18929e-05 
0 
0 
4.04396e-05 
1.63924e-05 
1.17895e-06 
4.14755e-05 
2.62577e-06 
1.02037e-06 
1.46666e-07 
1.17872e-06 
0 
2.65515e-08 
9.3971e-06 
0 
5.68551e-07 
3.6194e-05 
1.58024e-06 
9.97348e-08 
0 
0 
3.85733e-06 
6.0901e-06 
4.47441e-07 
8.13248e-06 
6.0901e-06 
2.10313e-06 
2.39249e-06 
3.43713e-07 
7.42101e-06 
0 
3.51397e-07 
1.39838e-07 
3.89874e-06 
0 
0 
0 
9.04427e-07 
6.4177e-07 
0 
6.38464e-05 
0.000231282 
0.000493575 
1.23793e-06 
3.75776e-07 
1.84738e-08 
1.43495e-06 
4.48845e-07 
1.92383e-07 
1.16076e-07 
1.85662e-05 
3.88436e-07 
9.85563e-08 
8.28216e-07 
3.51235e-06 
5.12439e-07 
5.37038e-06 
0 
2.39504e-07 
2.8524e-07 
2.8524e-07 
2.8524e-07 
2.8524e-07 
1.02687e-05 
2.9047e-07 
7.21187e-08 
3.77448e-05 
1.19447e-06 
5.92216e-07 
5.4001e-06 
0 
5.27331e-07 
2.24644e-08 
0 
2.8524e-07 
0 
1.38911e-06 
0 
1.58024e-06 
0 
4.31477e-05 
6.68347e-08 
2.9047e-07 
2.9047e-07 
6.32096e-06 
1.15142e-06 
1.15142e-06 
4.64752e-06 
2.8524e-07 
0 
4.31462e-06 
3.89369e-07 
0 
8.10821e-07 
1.03186e-06 
0 
0 
1.10039e-08 
7.96515e-07 
0 
0 
2.13452e-08 
3.79346e-08 
7.2711e-08 
2.9047e-07 
0 
4.87514e-08 
0 
1.89777e-06 
7.17993e-05 
0.000235523 
0.000458793 
1.5491e-07 
1.23464e-06 
6.26915e-07 
1.59272e-07 
3.88451e-08 
1.16188e-06 
0 
1.51971e-08 
# Learning phase...
 - Before training
Accuracy: 7163/9950 = 0.719899
Average error toward 0: -0.000188504 (1744)
Average error toward 1: 9.03647e-05 (1043)
Prev: 0.444724 (error: 0.175276) Offset: 0 0
Ratio error 1 : 0.374238
-----------------
- 3781 - 1043 - 
-----------------
- 1744 - 3382 - 
-----------------
 - Phase 1
Accuracy: 8921/9950 = 0.896583
Average error toward 0: -0.000201 (525)
Average error toward 1: 0.000100 (504)
Prev: 0.444724 (error: 0.052764) Offset: 0.000000 0.000000
Ratio error 1 : 0.489796
-----------------
- 5000 - 504 - 
-----------------
- 525 - 3921 - 
-----------------
 - Phase 2
Accuracy: 9176/9950 = 0.922211
Average error toward 0: -0.000202 (390)
Average error toward 1: 0.000101 (384)
Prev: 0.444724 (error: 0.039196) Offset: 0.000000 0.000000
Ratio error 1 : 0.496124
-----------------
- 5135 - 384 - 
-----------------
- 390 - 4041 - 
-----------------
 - Phase 3
Accuracy: 9230/9950 = 0.927638
Average error toward 0: -0.000203 (360)
Average error toward 1: 0.000102 (360)
Prev: 0.444724 (error: 0.036181) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 5165 - 360 - 
-----------------
- 360 - 4065 - 
-----------------
 - Phase 4
Accuracy: 9250/9950 = 0.929648
Average error toward 0: -0.000204 (350)
Average error toward 1: 0.000103 (350)
Prev: 0.444724 (error: 0.035176) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 5175 - 350 - 
-----------------
- 350 - 4075 - 
-----------------
 - Phase 5
Accuracy: 9259/9950 = 0.930553
Average error toward 0: -0.000204 (345)
Average error toward 1: 0.000103 (346)
Prev: 0.444724 (error: 0.034673) Offset: 0.000000 0.000000
Ratio error 1 : 0.500724
-----------------
- 5180 - 346 - 
-----------------
- 345 - 4079 - 
-----------------
 - Verification
Accuracy: 9364/9950 = 0.941106
Average error toward 0: -0.000204 (248)
Average error toward 1: 0.000103 (338)
Prev: 0.444724 (error: 0.024925) Offset: 0.000000 0.000000
Ratio error 1 : 0.576792
-----------------
- 5277 - 338 - 
-----------------
- 248 - 4087 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 749 0.975968
0.909353
Accuracy: 1055/1104 = 0.955616
Average error toward 0: -0.000000 (25)
Average error toward 1: 0.000000 (24)
Prev: 0.444724 (error: 0.002513) Offset: 0.000000 0.000000
Ratio error 1 : 0.489796
-----------------
- 607 - 24 - 
-----------------
- 25 - 448 - 
-----------------
# Prediction serialization...
# Saving the [1104,738] weight matrix...
4.574228
