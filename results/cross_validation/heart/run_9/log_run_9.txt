# Loading the instance files...
# Perform sanity checks on the dataset
# Setting the parameters...
# Initialize...
# Online: 0
# Verbose level: 0
# Cases: 270
# Total features: 3779
# Unique features: 385 (ratio: 0.101879)
# Minimum case size: 13
# Maximum case size: 14
# Average case size: 13.9963
# Add cases...
# Model serialization...
# Saving the [243,355] weight matrix...
# Calculate intrinsic strength...
Calculate strength for 243
Serialization of pre-training strength vectors...
1.47765e-05 
0.000245904 
0 
0.0819052 
0.0135482 
0.000621467 
0 
0.00448198 
0.0370685 
6.10927e-05 
0.0277072 
0.0193604 
0 
0.064304 
0.127484 
0.00436839 
0 
0.00804951 
0.176492 
0.0991948 
1.30194e-05 
0.000408248 
0.0782941 
0.05367 
5.96883e-05 
0 
0.00409586 
0 
0.128151 
0.000368185 
0.0112678 
0 
5.85387e-05 
0.0012412 
1.45168e-05 
0.00448951 
0.0196928 
6.23502e-05 
5.69954e-05 
0 
0.00150869 
1.37274e-05 
0 
1.49221e-05 
0 
0.000943762 
1.59992e-05 
6.29424e-05 
0.000156903 
0.00302863 
0.000126128 
0.000507176 
0.000132416 
1.44018e-05 
1.58044e-05 
5.93359e-05 
0 
0.000898506 
1.31895e-05 
0 
0.000218376 
0.000132779 
1.30194e-05 
6.14546e-05 
0 
0.00236096 
0.000492829 
0.000532496 
1.32472e-05 
0.00208973 
0.000245136 
1.70179e-05 
0.000126835 
0.000900384 
0.000232717 
5.43208e-05 
0.000564752 
1.30194e-05 
0 
0.000237033 
5.91066e-05 
1.48247e-05 
0 
0 
0.000904928 
0.000404639 
0 
0.000122368 
6.07179e-05 
0 
0 
1.50456e-05 
0.000237582 
0.000127619 
0.000243032 
1.61988e-05 
6.77578e-05 
0.000777905 
7.8399e-05 
6.05735e-05 
0 
1.35231e-05 
1.35231e-05 
0.000122451 
5.7546e-05 
5.87179e-05 
1.47048e-05 
0.000146111 
1.59431e-05 
1.58044e-05 
1.97127e-05 
0.000752631 
1.97127e-05 
1.47765e-05 
1.30756e-05 
0 
1.30944e-05 
5.64286e-05 
0.000726895 
6.27192e-05 
0.000242223 
1.65831e-05 
0.000127444 
1.36655e-05 
0.000229519 
1.3645e-05 
0 
0.000249474 
0.00020745 
1.30756e-05 
0.000132239 
5.9362e-05 
0 
0 
0.000356528 
0.000239552 
1.31323e-05 
6.17248e-05 
1.40024e-05 
0 
0 
0 
6.00209e-05 
1.40674e-05 
0.000368942 
1.67358e-05 
1.30381e-05 
0 
6.26837e-05 
0 
6.50032e-05 
1.60842e-05 
5.38521e-05 
0.000368885 
0 
6.42108e-05 
5.55142e-05 
5.53624e-05 
0.000527259 
0 
0 
1.47048e-05 
0 
0.000224888 
1.47765e-05 
1.82848e-05 
0.000134497 
0 
5.13999e-05 
0 
0 
5.29458e-05 
0 
6.62323e-05 
1.34035e-05 
6.01083e-05 
1.50456e-05 
0 
0.000242984 
1.31704e-05 
0 
1.48247e-05 
1.50456e-05 
1.40674e-05 
0 
0.000136698 
6.34771e-05 
0.00101551 
0 
1.71463e-05 
0 
0 
0 
1.3645e-05 
0 
0 
1.65831e-05 
0.000232204 
1.36041e-05 
1.68913e-05 
0.000129118 
0.000361757 
5.95436e-05 
1.60842e-05 
0.000129804 
0 
0.00023296 
5.84408e-05 
5.69752e-05 
0.000134795 
6.27661e-05 
1.52989e-05 
1.54026e-05 
0 
1.31513e-05 
2.44947e-05 
1.30944e-05 
0 
0 
6.30054e-05 
6.06339e-05 
5.52055e-05 
1.68913e-05 
0 
1.78187e-05 
6.09033e-05 
0.000139898 
1.39808e-05 
6.83407e-05 
1.33444e-05 
5.23544e-05 
5.45474e-05 
0 
0.000250814 
0 
0.000584595 
6.67058e-05 
1.36655e-05 
1.58873e-05 
0 
0 
0.000128021 
1.28174e-05 
0 
1.50456e-05 
2.44947e-05 
5.23795e-05 
0 
2.03301e-05 
6.64537e-05 
5.76878e-05 
1.40674e-05 
0 
1.6705e-05 
1.77491e-05 
1.77491e-05 
5.21798e-05 
5.42036e-05 
6.20695e-05 
0 
0 
6.24424e-05 
0 
0 
0 
5.61839e-05 
0 
0 
0 
1.41993e-05 
0 
1.51207e-05 
0.000131578 
0 
0 
5.43351e-05 
5.58041e-05 
1.47525e-05 
0.000134589 
0.00012868 
0 
1.4681e-05 
6.24274e-05 
5.5137e-05 
0 
1.36655e-05 
0 
0 
1.4379e-05 
1.4379e-05 
5.50691e-05 
1.28719e-05 
1.31704e-05 
1.31704e-05 
1.48247e-05 
0 
0.000131632 
5.17809e-05 
5.67972e-05 
1.33053e-05 
0 
1.30944e-05 
0 
5.79036e-05 
0 
6.63386e-05 
0 
6.33659e-05 
1.47286e-05 
0 
0 
0 
5.88298e-05 
1.33053e-05 
0 
1.94178e-05 
1.94178e-05 
0 
1.30381e-05 
1.36861e-05 
0 
5.61812e-05 
1.27814e-05 
0 
0 
6.37587e-05 
0.00015912 
5.71955e-05 
1.53506e-05 
5.44385e-05 
1.44018e-05 
0 
0 
1.44018e-05 
1.6705e-05 
6.31296e-05 
1.37274e-05 
0 
1.59151e-05 
0 
0 
1.48247e-05 
0 
1.686e-05 
1.47286e-05 
1.47286e-05 
1.31513e-05 
2.03301e-05 
0 
6.92797e-05 
1.5222e-05 
0 
0 
0 
0 
# Learning phase...
 - Before training
Accuracy: 203/243 = 0.835391
Average error toward 0: -0.000583698 (21)
Average error toward 1: 0.000564827 (19)
Prev: 0.452675 (error: 0.0864198) Offset: 0 0
Ratio error 1 : 0.475
-----------------
- 112 - 19 - 
-----------------
- 21 - 91 - 
-----------------
 - Phase 1
Accuracy: 205/243 = 0.843621
Average error toward 0: -0.000970 (20)
Average error toward 1: 0.001036 (18)
Prev: 0.452675 (error: 0.082305) Offset: 0.000000 0.000000
Ratio error 1 : 0.473684
-----------------
- 113 - 18 - 
-----------------
- 20 - 92 - 
-----------------
 - Verification
Accuracy: 208/243 = 0.855967
Average error toward 0: -0.001142 (16)
Average error toward 1: 0.001332 (19)
Prev: 0.452675 (error: 0.065844) Offset: 0.000000 0.000000
Ratio error 1 : 0.542857
-----------------
- 117 - 19 - 
-----------------
- 16 - 91 - 
-----------------
Serialization of post-training strength vectors...
# Predictions
# Already in case-base: 0 -nan
0.675000
Accuracy: 22/26 = 0.846154
Average error toward 0: -0.000006 (2)
Average error toward 1: 0.000078 (2)
Prev: 0.452675 (error: 0.008230) Offset: 0.000000 0.000000
Ratio error 1 : 0.500000
-----------------
- 14 - 2 - 
-----------------
- 2 - 8 - 
-----------------
# Prediction serialization...
# Saving the [26,355] weight matrix...
0.063416
